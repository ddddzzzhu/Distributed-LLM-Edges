% 逻辑主线: 通过分析当前大模型发展面临的瓶颈，提出以联邦学习和边缘设备共同训练模型的必要性。

\section{Introduction}
% \begin{insights}
%     \textbf{insights:}
%     \begin{itemize}
%         \item 111
%     \end{itemize}
% \end{insights}
% 1.1 大模型与Scaling Law的驱动力
% \subsection{Foundation Models and the Driving Force of Scaling Laws}
% - 为什么大模型能成功？（Scaling Law如何推动模型性能提升？）
% - Scaling Law依赖的核心是什么？（模型规模和训练数据规模的扩张）
% - 进一步突破Scaling Law需要满足哪些条件？（更广泛的数据与更强的算力）

\textbf{Scaling laws} \citep{kaplan2020scaling,hoffmann2022training} have been fundamental to the remarkable success of foundation models, demonstrating a predictable relationship between performance and the expansion of model parameters and training data. These laws have guided the development of increasingly powerful models, from BERT \citep{devlin2018bert} to GPT-4 \citep{openai2023gpt4}, showing that performance improvements can be achieved through systematic scaling of both model size and training data \citep{brown2020language,chowdhery2022palm}. However, the continued application of these scaling laws requires ever-increasing amounts of data and computational resources, pushing the boundaries of what is currently feasible \citep{patterson2021carbon}. 

% 1.2 公共数据的日益枯竭
% \subsection{The Growing Depletion of Public Data}
% - 公共数据为什么重要？（它是训练大模型的主要"燃料"）
% - 为什么说公共数据正在枯竭？（质量和数量的饱和、数据增量不足）
% - 如果公共数据枯竭，可能带来的后果是什么？（制约模型性能的提升）

\textbf{Public data} has been the primary \textit{fuel} driving AI development forward. 
This field has witnessed an exponential growth in data requirements, from the early success of MNIST \citep{lecun1998mnist} with its 70,000 handwritten digits to ImageNet's revolutionary impact with 14 million labeled images \citep{deng2009imagenet}. 
This trajectory has continued with modern large language models (LLMs) like GPT \citep{gpt}, LLaMA \citep{llama}, and DeepSeek \citep{liu2024deepseek} series, which are trained on trillions of tokens.
Recent evidence that LLaMA 3.1's smallest model (8B) \citep{meta2024llama3.1} trained on 15 trillion tokens, outperforms LLaMA 2's largest model (70B) \citep{touvron2023llama} trained on 2 trillion tokens (despite being $10 \times$ smaller in model size, the $7 \times$ increase in training data leads to superior performance), demonstrates the paramount importance of data scaling \citep{sun2017revisiting,raffel2020exploring, deeplearningai2024federated}. 
However, we are witnessing a concerning trend of data depletion, where high-quality public data sources are becoming exhausted \citep{lee2021dedup, biderman2022data}. \citet{villalobosposition} argues that human-generated public text data cannot sustain scaling beyond this decade. 
While recent efforts advocate for training larger models with synthetic data \citep{chen2024diversity}, AI-generated content may fail to yield performance improvements \citep{wenger2024ai}, also risks polluting public data sources \citep{fang2024bias}. 
Moreover, stricter data privacy regulations like GDPR \citep{gdpr} have made data collection increasingly difficult and expensive.
This looming data scarcity suggests that scaling laws may hit a wall \citep{hardy2024wall}, potentially impeding further AI advancement.


% 1.3 算力垄断的困境
% \subsection{The Dilemma of Computing Power Monopoly}
% - 当前AI算力分布的现状如何？（算力正在向少数大公司集中）
% - 算力垄断会带来哪些问题？（成本高昂、创新壁垒、民主化受阻）
% - AI发展的算力扩张是否遇到了生物或物理限制？（摩尔定律放缓、芯片效能瓶颈）


\textbf{Computational resources} has been the primary \textit{engine} powering AI development. Throughout AI history, major breakthroughs have been closely tied to advances in computing power, from early models requiring single CPUs (with peak performance of 1-2 GFLOPS) to modern GPU clusters. The computational demands have grown exponentially - from BERT-Large's training requiring 64 TPU v3 chips (providing 420 TFLOPS) \citep{devlin2018bert} to GPT-3's training on 10,000 V100 GPUs (reaching 28,000 TFLOPS) \citep{brown2020language}, while training GPT-4 reportedly required over 25,000 NVIDIA A100 GPUs (delivering a staggering 400,000 TFLOPS) \citep{openai2023gpt4}. More recent models like Grok 3 push these requirements even further \citep{xai2025grok3}. However, we are approaching physical limits in single-chip performance as Moore's Law slows down \citep{thompson2021deep}. While massive computing clusters can compensate for individual chip limitations, maintaining such infrastructure incurs astronomical costs - estimated at over \$100M for training GPT-4 \citep{sharir2020cost} - and poses significant environmental concerns due to their enormous energy consumption, with each training run emitting as much CO$_2$ as 500 cars driven for a year \citep{schwartz2020green}. Moreover, this level of computing power has become concentrated among a few tech giants, creating a monopolistic landscape that effectively excludes smaller companies and academic institutions from participating in foundational AI research \citep{ahmed2022democratizing}. This centralization of computing resources presents a significant barrier to innovation and democratization in AI development \citep{thompson2022frontier}.

% 1.4 研究目标
% \subsection{Research Objectives}
% - 现有模式的缺陷是什么？（集中式大模型的局限性）
% - 为什么联邦学习和小设备的协同训练可以成为替代方案？
% - 本文的研究重点和主张是什么？（利用分布式边缘设备与联邦学习技术打破瓶颈）

% However, massive edge devices offer potential distributed pools of data and computing power for training large models, with total scale estimated to far exceed current centralized capabilities. In this position paper, \textbf{we argue that leveraging these massive distributed edge devices can break barriers of data and computing wall, and everyone can participate in training large models with small edge devices}. 
% In this paper, we propose a paradigm shift: leveraging massive distributed edge devices as an alternative pathway to break through both data and computing barriers in AI development.

In this paper, we propose that leveraging massive distributed edge devices offers a promising solution to overcome both data and computing barriers in AI development. 
Our analysis reveals two compelling opportunities: 
First, edge data generated from smartphones for past 5 years are projected to reach 33.1 EB, offering fresh, diverse, and contextually rich training samples. 
Second, the collective computing power of edge devices - with smartphones delivering 9,278 EFLOPS for past 5 years - demonstrates the feasibility of distributed model training, as training state-of-the-art models like DeepSeek-v3 would require only about 60,723 users with edge devices working (\textit{ideally}) in parallel to match its current training setup. 
Based on these insights, \textbf{we argue that leveraging these massive distributed edge devices can break barriers of data and computing wall, and everyone can participate in training large models with small edge devices.} 
To support this position, we first analyze the critical challenges of large language models, examining both data bottlenecks (\S\ref{subsec:data_exhaustion}) and computational monopolization (\S\ref{subsec:compute_monopoly}). 
We then explore the hidden potential of massive edge devices, investigating their vast untapped distributed data resources (\S\ref{subsec:edge_data}) and computational capabilities (\S\ref{subsec:edge_compute}). 
Building on these insights, we investigate technical approaches for overcoming large model challenges through distributed computing architectures (\S\ref{sec:technical_advancements}): small language models at edges (\S\ref{subsec:small_language_models}), collaborative inference (\S\ref{subsec:collaborative_inference}), and collaborative training (\S\ref{subsec:collaborative_training}).
% We then identify two critical open challenges (\S\ref{sec:open_problem}): heterogeneous device model fusion (\S\ref{subsec:model_fusion}) and heterogeneous device compute sharing (\S\ref{subsec:compute_sharing}) for edge devices collaboration. 
% Finally, we conclude by discussing the profound societal implications like AI democratization (\S\ref{subsec:ai_monopoly_and_democratization}), incentive mechanisms(\S\ref{subsec:fairness_and_incentive_mechanisms}), and environmental benefits of this paradigm shift (\S\ref{subsec:carbon_footprint_and_energy_efficiency}).
We then identify two critical open challenges: heterogeneous device model fusion and heterogeneous device compute sharing (\S\ref{sec:open_problem}). 
Finally, we discuss the societal impact like AI democratization, incentive mechanisms, and environmental benefits of this paradigm shift (\S\ref{sec:impacts}).


