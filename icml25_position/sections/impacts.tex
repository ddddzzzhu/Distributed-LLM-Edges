% 逻辑主线: 通过终端设备的协作训练打破垄断，让AI更加普惠和多样化。

% 第6章：广泛影响
\section{Impact Statements}\label{sec:impacts}

The shift from centralized to distributed training of large models, may introduce new technical and societal challenges and have the potential to fundamentally reshape the AI landscape. 

% 6.1 AI垄断与民主化
\subsection{AI Monopoly and Democratization}\label{subsec:ai_monopoly_and_democratization}
% - 终端设备参与训练如何冲击算力垄断？
% - 这种去中心化模式是否有助于普惠性创新？

The current AI landscape is characterized by significant concentration of power among a few tech giants, primarily due to their monopoly over massive computing resources and data centers \cite{bommasani2021opportunities}. This monopolistic trend has intensified with companies like OpenAI increasingly moving towards closed systems. 
While open-source alternatives like Llama \cite{touvron2023llama}, Deepseek \cite{liu2024deepseek} and other community-driven models have made strides towards democratization by releasing model parameters and technical reports \cite{democratizing2024ai}, the gap in computational resources and data access between major AI companies and other players remains substantial and continues to widen. 
This disparity in resources allows tech giants to maintain their absolute dominance in determining the direction of AI development, raising concerns about AI democratization. 


Edge device-based collaborative training presents a promising pathway to democratize AI development \cite{collaborative2024edge}.
By leveraging the collective computing power of millions of edge devices, this approach could effectively challenge the existing monopolistic structure \cite{community2024driven, distributed2024training}. 
This democratization of AI training through edge devices could fundamentally reshape the structure of responsibilities and authorities.
If everyone can participate in training LLMs, the AI landscape could fundamentally change. Training decisions would shift from companies to communities, creating shared responsibility for model development \cite{decentralized2024llm}. Global participation would help models reflect diverse cultural perspectives, while allowing communities to adapt models for their local needs.
Furthermore, this decentralized approach could foster a more competitive and innovative AI ecosystem. When the barriers to entry for AI model training are lowered, we can expect to see a broader range of specialized models emerging (like \cite{domain2024survey,medical2024llm,finance2024gpt,legal2024transformer,science2024llm,education2024transformer}), better suited to local needs and diverse use cases.

% % 6.2 数据隐私
% \subsection{Data Privacy}
% % - 用户个人隐私能否通过联邦学习得到保证？
% % - 隐私保护的机制是否会对模型性能产生影响？

% Privacy preservation is a cornerstone benefit of edge device-based training \cite{bonawitz2017practical}. Through federated learning techniques, users' data remains on their local devices while contributing to model improvement \cite{mcmahan2017communication}. This approach addresses one of the most pressing concerns in AI development - the protection of personal information. The local processing of data eliminates the need for raw data transmission to central servers, significantly reducing privacy risks \cite{truex2019hybrid}.

% While privacy-preserving mechanisms like differential privacy \cite{dwork2014algorithmic} and secure aggregation \cite{bonawitz2017practical} may introduce some computational overhead, research has shown that these trade-offs are often manageable. Modern techniques can maintain model performance while providing strong privacy guarantees, striking a balance between utility and privacy protection \cite{wei2020federated}.

% % 6.3 个人与社会效益
% \subsection{Individual and Social Benefits}
% % - 用户端贡献数据能否获得反哺？（更智能化的服务）
% % - 替代目前集中式模型训练后，AI对社会进步的影响有哪些？

% The distributed training paradigm creates a virtuous cycle where users' contributions to model training directly translate into improved personalized services \cite{wang2019edge}. Users benefit from models that better understand their specific needs and usage patterns, while maintaining control over their data \cite{li2020federated}. This personalization can lead to more efficient and effective AI applications across various domains, from healthcare to education \cite{rieke2020future}.

% From a broader societal perspective, the shift away from centralized training could lead to more equitable access to AI technology \cite{kairouz2021advances}. This democratization could help bridge the digital divide, enabling communities worldwide to develop and deploy AI solutions that address their specific challenges and needs.

% 6.4 公平性与激励机制
\subsection{Fairness and Incentive Mechanisms}\label{subsec:fairness_and_incentive_mechanisms}
% - 讨论分布式训练的模型的公平性？
% - 是否有激励机制吸引用户或设备参与模型训练？

The distributed training paradigm introduces new considerations for model fairness and bias mitigation \cite{yao2024pursuit,li2020fair}. When training occurs across diverse edge devices, the resulting models can potentially better reflect the heterogeneous nature of user populations \cite{wang2020optimizing}. However, this approach also raises concerns about participation bias, where differences in device capabilities or user engagement could lead to underrepresentation of certain groups \cite{kairouz2021advances}. To address these challenges, researchers have proposed various fairness-aware federated learning algorithms \cite{mohri2019agnostic} that aim to ensure equitable model performance across different demographic groups and device types \cite{li2020fair}.

To sustain a distributed training ecosystem, effective incentive mechanisms are crucial for motivating user participation \cite{kang2019incentive}. Traditional approaches like computational resource sharing \cite{khan2019federated} and privacy-preserving reward systems \cite{zhan2020learning} have shown promise in encouraging user engagement. More innovative solutions include token-based reward systems \cite{incentive2024blockchain} and reputation mechanisms \cite{yang2019federated} that compensate users for their contributions while maintaining system integrity. These incentive structures not only encourage consistent participation but also help ensure the quality of contributed training data \cite{feng2019learning}, creating a sustainable ecosystem for collaborative AI development.

\subsection{Carbon Footprint and Energy Efficiency}\label{subsec:carbon_footprint_and_energy_efficiency}
The shift from centralized to distributed training offers compelling environmental benefits \cite{yang2024environmental}. Traditional data centers housing large language models face significant energy challenges\cite{li2024carbon} - their high-performance GPUs require extensive cooling systems that consume 30-40\% of total energy \cite{cooling}. In contrast, FL distributes computation across edge devices like smartphones and tablets that operate at much lower temperatures and power levels, eliminating industrial cooling needs \cite{fl_vs_centralized}.

FL also dramatically reduces data transmission energy costs. While centralized approaches require raw data transfer from millions of devices, FL only transmits lightweight model updates, substantially decreasing network energy overhead \cite{fl_data_transmission}. The hardware efficiency gap is striking - edge devices like the NVIDIA Tegra X2 consume just 7.5W during training compared to 250W for data center GPUs \cite{hardware_power}, translating to major carbon footprint reductions, particularly for simpler models \cite{fl_iid}.
By reducing reliance on power-hungry data centers and leveraging existing consumer devices, FL enables more sustainable AI development through optimized energy efficiency and minimized infrastructure needs. This combination of reduced cooling requirements, efficient hardware utilization, and optimized data handling makes FL an environmentally responsible choice for the future of AI training \cite{fl_environmental}. As climate impact becomes increasingly critical, FL's sustainability advantages position it as a key technology for green AI development.
