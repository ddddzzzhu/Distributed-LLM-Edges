% 逻辑主线: 梳理现有AI发展依赖的关键要素和遇到的瓶颈，论述为什么海量小设备可以成为未来突破点。

% 第2章：大语言模型是AI发展的历史性成功
\section{Historical Development and Current Challenges}\label{sec:history}
% - 大语言模型是AI发展的历史性成功

% 2.1 大模型成功的驱动力: Scaling Law
% \subsection{Scaling law: the compass for LLMs}
% - 什么是Scaling Law？
% - 图2.1来说明Scaling Law，大模型的参数，以及训练所需的数据量、算力和越来越大，比如从BERT到GPT-4
% - Scaling Law为什么推动了大模型的成功？（模型/数据的扩张如何保障性能提高？）
% - 数据、模型、算力的扩张之间的关系。

% 2.2 海量数据是大模型成功的核心原因
\subsection{Data: the fuel of LLMs}

\paragraph{Early data-driven AI development}
As LLMs continue to achieve unprecedented success in artificial intelligence, understanding the role of data becomes increasingly crucial. From the early days of simple datasets to the modern era of massive data collections, data has consistently served as the lifeblood of AI, determining the upper bounds of model capabilities. The evolution of AI—marked by breakthroughs in computer vision, natural language processing, and beyond—can be traced back to the continuous expansion and refinement of data resources. 

In the early stages of AI, despite relatively small data scales, the importance of data was already evident. The MNIST dataset, for instance, serves as a notable example. With 60,000 training images and 10,000 test images, it provided a crucial foundation for neural network research, demonstrating the fundamental role of data in model training~\cite{lecun1998mnist}. As data scales expanded, the capabilities of deep learning models saw significant improvements. The emergence of ImageNet, which contains 14 million images across 21,000 synsets, revolutionized computer vision. This enabled deep learning models like AlexNet to learn complex visual features and achieve breakthrough progress in image recognition tasks, reducing error rates from 26.2\% to 15.3\% in the ILSVRC-2012 competition~\cite{deng2009imagenet,krizhevsky2012imagenet}. ImageNet's success stemmed not only from its scale but also from its high quality and diversity, laying the groundwork for subsequent large-scale data applications.

\paragraph{Era of massive data}
With the proliferation of the internet and advances in computing power, data scales have expanded dramatically, ushering AI into an era of massive data. GPT-3, for instance, was trained on 450 billion tokens, with a carefully curated mix of data sources: Common Crawl (60\%), books (16\%), Wikipedia (3\%), and other internet-based text (21\%)~\cite{brown2020language}. This massive dataset enabled GPT-3 to excel across various tasks, demonstrating the decisive role of data scale in model capabilities. Compared to early datasets like MNIST and ImageNet, GPT-3's data scale and quality reached unprecedented heights, not only advancing natural language processing but also opening new possibilities for AI generalization.

\paragraph{Quality and diversity matter}
Beyond scale, data quality and diversity are crucial factors in model performance. ImageNet ensures data quality through rigorous validation, with each image verified by an average of 3.3 annotators and achieving 95\% accuracy in its labels~\cite{deng2009imagenet}. This precise annotation enables models to learn accurate visual features and excel in image classification tasks. In the realm of large language models, GPT-3's training data underwent stringent cleaning and filtering, including deduplication, quality scoring based on document length and linguistic complexity, and content filtering for inappropriate content~\cite{brown2020language}. This high-quality data enables GPT-3 to generate coherent and accurate text. Furthermore, diversity is essential: ImageNet covers 1,000 object categories across various domains, while GPT-3's training data spans multiple languages, genres, and knowledge domains, providing rich linguistic knowledge and contextual understanding.

\paragraph{Data as the ceiling for model capabilities}
A model's capability depends on the knowledge it extracts from data, following empirically observed scaling laws. While increasing model parameters can enhance expressive power, without sufficient data, models cannot effectively utilize these parameters. DeepMind's research on the Chinchilla model demonstrated that under the same compute budget, a 70B parameter model trained on 1.4T tokens outperforms a 280B parameter model trained on 0.35T tokens, achieving a 30\% reduction in loss while using the same compute resources~\cite{hoffmann2022training}. This finding directly supports the notion that data acts as a ceiling for model capabilities. Additionally, Meta's research shows that while Llama 2 (70B) has 70 billion parameters, its performance largely benefits from training on 2T tokens of high-quality data, with particular emphasis on academic papers, code repositories, and books that enhance its reasoning capabilities~\cite{touvron2023llama}. These studies emphasize data's central role in model training and suggest that optimal model scaling requires a balanced increase in both parameters and training data.

\paragraph{Looking ahead}
From MNIST to ImageNet to GPT-3, advances in data scale, quality, and diversity have directly driven AI breakthroughs. Data remains the foundation of AI development, determining the upper limits of model capabilities. As we push the boundaries of LLM performance, the challenge of acquiring sufficient high-quality, diverse data becomes increasingly acute. Traditional data sources like the internet are showing signs of exhaustion, and concerns about data privacy and ownership are growing. This motivates the exploration of novel data acquisition approaches, such as leveraging edge devices and distributed data collection, which we will explore in subsequent sections. The future of LLMs may depend not just on scaling existing data sources, but on fundamentally rethinking how we collect, curate, and utilize data in AI training.

% 2.3 大模型的训练需要超大算力支撑
\subsection{Computing power: the engine of LLMs}
% - 历史上算力的提升如何推动AI的突破？
% - 算力发展的历史（从CPU到GPU到TPU，从单卡到多卡到集群）
% - 图2.1解释所需算力不断增大（从BERT到GPT-4需要算力的变化）
% - AI的每次进步都是靠算力支撑。
\paragraph{Early neural networks and CPU era}
Since the inception of neural networks, every breakthrough in the field of AI has been driven by the continuous improvement of computational power~\cite{thompson2020computational}. From the early multilayer perceptron (MLP) to the widely used large language models (LLM) today, the progress in computing power has always been a key engine for advancing AI.

As the prototype of neural networks, the MLP was initially used to solve linearly separable problems~\cite{rosenblatt1958perceptron}. Due to its relatively low computational demand, it could run on traditional CPU environments. However, as the complexity of neural network models increased and application scenarios expanded, computational requirements gradually rose. The emergence of Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) marked a surge in computational demands. CNN, through convolutional operations, effectively reduced the number of parameters, enhancing the computational efficiency of image processing tasks. Classic models such as LeNet~\cite{lecun1998gradient} and AlexNet~\cite{krizhevsky2012imagenet} achieved significant results in image classification, but this also led to a surge in computational resource demands. For example, AlexNet's victory in the 2012 ImageNet competition was made possible by using the NVIDIA GTX 580 GPU, which significantly boosted computational performance~\cite{krizhevsky2012imagenet}.

\paragraph{GPU and TPU revolution}
With the growing scale of neural network models, GPUs gradually became indispensable computing tools~\cite{raina2009large}. The parallel computing capabilities of GPUs greatly accelerated the training process of neural networks, particularly in the field of deep learning. Meanwhile, specialized hardware for deep learning, such as Tensor Processing Units (TPUs), emerged~\cite{jouppi2017datacenter}. Compared to GPUs, TPUs offer higher efficiency and lower power consumption when performing matrix operations and deep learning tasks~\cite{wang2019benchmarking}, making them the preferred hardware for training large-scale neural networks.

\paragraph{Transformer era and computational demands}
As computational resources continued to expand, the scale of neural network model training also grew. The introduction of the Transformer architecture~\cite{vaswani2017attention} revolutionized the field of natural language processing (NLP), especially with the launch of models like BERT~\cite{devlin2018bert} and the GPT series~\cite{brown2020language,openai2023gpt4}, which pushed NLP technology to new heights. However, the self-attention mechanism in the Transformer architecture has a computational complexity of $O(n^2)$, where n represents the sequence length~\cite{vaswani2017attention}. This means that as the model scale and sequence length increase, the required computational power grows exponentially. For example, training large language models like GPT-3~\cite{brown2020language} and GPT-4~\cite{openai2023gpt4} involves trillions of parameters and requires thousands of GPUs or TPU nodes to support the process. This immense computational demand not only places extremely high requirements on hardware, but also on computational frameworks, storage, and communication bandwidth, creating unprecedented challenges~\cite{patterson2021carbon}.

\paragraph{Computing power as the key driver}
Every leap in Artificial Intelligence has been driven by computational power~\cite{amodei2018ai}. From multilayer perceptrons to convolutional neural networks, and the introduction of the Transformer architecture, every innovation in models has been accompanied by an explosive growth in computational needs~\cite{thompson2020computational}. Particularly in the era of large language models, computational power is not only the foundational tool for model training but also the core driving force behind breakthroughs in AI performance~\cite{kaplan2020scaling}. The success of large-scale models like GPT-4 validates that AI progress almost entirely depends on the support of more powerful computational resources~\cite{hoffmann2022training}.



\section{Smartphone Data Volume Estimation}  
\label{app:smartphone_ethod}

In the absence of publicly available, granular data on per-user smartphone data generation patterns, we adopt a conservative estimation approach to approximate the total annual smartphone data volume. While this method necessarily involves simplifications, it provides a robust lower-bound approximation that is sufficient to support our core arguments without compromising the validity of our conclusions.

\textbf{Data volume estimation per smartphone}: Based on industry reports  \cite{counterpoint_smartphone_2021}, the average smartphone storage capacity reached 100 GB in 2020. To ensure a conservative estimate, we assume that only 1\% of this storage capacity (equivalent to approximately 1 GB per smartphone) is actively used for data generation and storage, including local images, video information, and other types of user-generated content. This assumption aligns with baseline usage scenarios while intentionally underestimating actual data utilization.
    % \item \textbf{Static Data Generation}: For simplicity, we adopt a static model that assumes no incremental data generation beyond this initial 1 GB allocation per device. This approach disregards dynamic factors such as daily usage patterns, application updates, or video/photo storage, thereby providing a conservative lower bound for smartphone data volume.

\textbf{Number of smartphones}: The growth of the number of smartphone users is an important basis for estimating the total amount of data. For this, we have referred to data from market research institutions \cite{bankmycell_smartphone_2023}, which includes trends in changes to the number of smartphone users over time.


Based on the above statistical data, the total annual smartphone data volume \( D_{\text{total}} \) is calculated using the following formula:  
\begin{equation}  
    D_{\text{total}}  (\text{EB}) = N_{\text{users}} \times 1 \, \text{GB/user} \times 10^{-3}  \, (\text{conversion from GB to EB}),
\end{equation}  
where \( N_{\text{users}} \) represents the global smartphone user base in billions.  

Substituting \( N_{\text{users}} = 8.0 \times 10^9 \) (representing 8 billion users) into Equation (1):  
\begin{equation*}  
    D_{\text{total}} = 8.0 \, \text{GB/user} \times 10^{-3} = 8.0 \, \text{EB}.
\end{equation*}  

Our purpose is to establish a defensible lower bound for analysis. Even under these stringent assumptions, the derived volumes remain orders of magnitude higher than synthetic or centralized datasets, thereby reinforcing the strategic importance and value of edge-generated data. This conservative estimation underscores the critical need for scalable solutions capable of managing and leveraging such vast quantities of distributed data effectively.  


\begin{table}[h!]
\centering
\caption{Trends in Smartphone Shipments and Compute Power. (Data source: \cite{canalys2025}).}
\label{tab:chip_total}
\resizebox{.95\linewidth}{!}{%
\begin{tabular}{cccc}
\hline
\textbf{Company} & \textbf{Shipments (Million units)} & \textbf{Chip Performance Range (TFLOPS)} & \textbf{Total Compute Power Contribution (EFLOPS)} \\
\hline
\multicolumn{4}{c}{\textbf{2020}} \\ \hline
Samsung (20\%) & 255.5 & 1.20--1.53 & 349 \\
Apple (16\%)   & 207.2 & 0.65 & 135 \\
Xiaomi (12\%)  & 149.6 & 0.24--1.20 & 108 \\
OPPO (9\%)    & 119.4 & 0.24--1.20 & 86 \\
vivo (9\%)    & 112.6 & 0.24--1.20 & 81 \\
Others (33\%)  & 420.5 & 0.04--0.24 & 59 \\ \hline
\multicolumn{4}{c}{\textbf{Overall: Shipments = 1265 Million, Compute Power = 817 EFLOPS}} \\ \hline \hline

\multicolumn{4}{c}{\textbf{2021}} \\ \hline
Samsung (20\%) & 274.5 & 1.42--1.72 & 430 \\
Apple (17\%)   & 230.1 & 1.71--1.94 & 420 \\
Xiaomi (14\%)  & 191.2 & 0.82--1.74 & 240 \\
OPPO (11\%)    & 145.1 & 0.82--1.74 & 180 \\
vivo (10\%)    & 129.9 & 0.82--1.74 & 160 \\
Others (28\%)  & 379.4 & 0.27--0.82 & 207 \\ \hline
\multicolumn{4}{c}{\textbf{Overall: Shipments = 1350 Million, Compute Power = 1637 EFLOPS}} \\ \hline \hline

\multicolumn{4}{c}{\textbf{2022}} \\ \hline
Samsung (22\%) & 257.9 & 0.49--2.01 & 322 \\
Apple (19\%)   & 232.2 & 1.79 & 416 \\
Xiaomi (13\%)  & 152.7 & 1.01--3.49 & 351 \\
OPPO (10\%)    & 113.4 & 1.01--3.49 & 261 \\
Transsion (6\%)     & 73.1 & 0.24--0.98 & 44.6 \\
Others (31\%)  & 364.1 & 0.84--1.31 & 393 \\ \hline
\multicolumn{4}{c} {\textbf{Overall: Shipments = 1193 Million, Compute Power = 1788 EFLOPS}} \\ \hline \hline

\multicolumn{4}{c}{\textbf{2023}} \\ \hline
Apple (20\%)   & 229.1 & 2.15 & 493 \\
Samsung (20\%) & 225.5 & 2.01--2.77 & 539 \\
Xiaomi (13\%)  & 146.1 & 2.15--3.99 & 449 \\
OPPO (9\%)     & 100.7 & 2.15--3.99 & 309 \\
Transsion (8\%)  & 92.6 & 0.24--1.31 & 72 \\
% vivo (7.6\%)  & 87.0 & 2.15--3.99 & 267 \\
Others (30\%)  & 347.9 & 0.24--2.15 & 416 \\
\hline
\multicolumn{4}{c}{\textbf{Overall: Shipments = 1142 Million, Compute Power = 2278 EFLOPS}} \\ \hline \hline

\multicolumn{4}{c}{\textbf{2024}} \\ \hline
Apple (18\%)   & 225.9 & 1.91--2.29 & 474 \\
Samsung (18\%) & 222.9 & 3.38--3.41 & 758 \\
Xiaomi (14\%)  & 168.6 & 3.38--4.95 & 703 \\
Transsion (9\%)  & 106.7 & 0.05--0.67 & 38  \\
OPPO (8\%)   & 103.6 & 3.38--4.95 & 432 \\
Others (33\%)  & 395.4 & 0.05--1.72 & 352 \\
\hline
\multicolumn{4}{c}{\textbf{Overall: Shipments = 1223 Million, Compute Power = 2758 EFLOPS}} \\ \hline
\end{tabular}%
}
\end{table}

\section{Estimation of Smartphone Total Computational Power}  
\label{app:total_computation}

To assess the (ideally) aggregate computational capabilities of smartphones globally, we estimate the total computing power, given the current lack of comprehensive statistical data in this domain. Our approach leverages two key data sources: the annual worldwide shipment volumes for major smartphone brands, and the computational performance specifications of mobile processors deployed in their devices during each corresponding year. The complete data underlying our analysis is presented in Table~\ref{tab:chip_total}, which provides a detailed breakdown by manufacturer and time period.
For quantitative analysis, we formulated a mathematical model to calculate the total computing power. Specifically, for any given year, we compute the aggregate computational capacity ($C_{\text{total}}$) by summing the contributions from each smartphone manufacturer ($i$). Each manufacturer's contribution is determined by multiplying their total device shipments ($N_i$) by the average computing power of their mobile processors ($P_i$) for that year, expressed formally as:

\begin{equation}
    C_{\text{total}} = \sum_{i} N_i \cdot P_i
\end{equation}

This formulation enables us to systematically track the evolution of distributed computing power across the smartphone ecosystem while accounting for both market share dynamics and technological advancement in mobile processors. By maintaining conservative estimates for processor capabilities and focusing on verified shipment data, our analysis provides a reliable lower bound for the total computational resources available through smartphones.

\section{Small Language Model (SLM) Architectures and Training Methods}
\label{app:slm_architectures_training}

Table~\ref{tab:slm_architectures_training} presents a comprehensive overview of the Small Language Model (SLM) landscape, categorized by architectures and training methodologies, according to \citet{wang2024comprehensive}. The table is organized into two main categories: (I) Transformer-Based Models, which represent the dominant architecture in current SLMs, and (II) Alternative Architecture Models, which explore novel approaches to achieve efficiency. The Transformer-Based section is further divided into models pre-trained from scratch, models derived from larger LLMs through knowledge distillation, and models created through various compression techniques (pruning, quantization, etc.). The Alternative Architecture section showcases emerging approaches like State Space Models (Mamba, Hymba), recurrent architectures (RWKV, xLSTM), and traditional encoder-decoder or encoder-only designs. 


\begin{table}[h!]
    \caption{Small Language Model (SLM) Architectures and Training Methods}
    \label{tab:slm_architectures_training}
    \tiny
    \begin{tabularx}{\textwidth}{p{2.5cm}p{1.5cm}p{1.5cm}ccp{2.5cm}p{3cm}}
    \toprule
    \textbf{Model} & \textbf{Sizes} & \textbf{Architecture} & \textbf{From Scratch} & \textbf{From LLMs} & \textbf{Training Method} & \textbf{Datasets} \\
    \midrule
    
    \multicolumn{7}{l}{\textbf{\textit{I. Transformer-Based Models}}} \\
    \midrule
    
    \multicolumn{7}{l}{\textit{I.A. Pre-Trained from Scratch}} \\
    \addlinespace[0.5ex]
    PhoneLM \cite{yi2024phonelm} & 0.5B; 1.5B & Transformer & \checkmark & & Pre-training & DCLM-baseline \cite{li2024datacomp}, StarCoderData \cite{li2023starcodersourceyou} \\
    Llama 3.2 \cite{llama3.2} & 1B; 3B & Transformer & \checkmark & & Pre-training, SFT, RLHF, DPO & Not released (9T tokens) \\
    Qwen 1/1.5/2/2.5 \cite{yang2024qwen2, bai2023qwentechnicalreport} & 0.5B-7B & Transformer & \checkmark & & Pre-training & Not released \\
    Gemma/Gemma 2 \cite{team2024gemma, team2024gemma2} & 2B; 7B & Transformer & \checkmark & & Pre-training & Unknown \\
    SmolLM \cite{allal2024SmolLM} & 135M-1.7B & Transformer & \checkmark & & Pre-training & SmolLM corpus \cite{benallal2024smollmcorpus} \\
    H2O-Danube3 \cite{pfeiffer2024h2o} & 500M; 4B & Transformer & \checkmark & & Pre-training (multi-stage) & Unknown \\
    MiniCPM \cite{hu2024minicpm} & 1.2B; 2.4B & Transformer & \checkmark & & Pre-training & Dolma \cite{dolma}, C4 \cite{raffel2020exploring} \\
    CT-LLM \cite{du2024chinesetinyllmpretraining} & 2B & Transformer & \checkmark & & Pre-training & MAP-CC \\
    OLMo \cite{groeneveld2024olmo} & 1B; 7B & Transformer & \checkmark & & Pre-training & Dolma \cite{dolma} (multiple sources) \\
    TinyLlama \cite{zhang2024tinyllamaopensourcesmalllanguage} & 1B & Transformer & \checkmark & & Pre-training & SlimPajama \cite{cerebras2023slimpajama} \\
    Phi-series \cite{abdin2024phi, javaheripi2023phi} & 1.3B-6.6B & Transformer & \checkmark & & Pre-training & CodeTextBook \cite{gunasekar2023textbooksneed} \\
    OpenELM \cite{mehta2024openelm} & 270M-3B & Transformer & \checkmark & & Pre-training & RefinedWeb \cite{penedo2023refinedweb}, PILE \cite{gao2020pile} \\
    MobiLlama \cite{thawakar2024mobillama} & 0.5B; 0.8B & Transformer & \checkmark & & Pre-training & LLM360 Amber \\
    MobileLLM \cite{liu2024mobilellm} & 125M; 350M & Transformer & \checkmark & & Pre-training & Unknown (1T tokens) \\
    \addlinespace[0.5ex]
    
    \midrule
    \multicolumn{7}{l}{\textit{I.B. Derived from Larger Models}} \\
    \addlinespace[0.5ex]
    MINITRON \cite{muralidharan2024compact} & 4B & Transformer & & \checkmark & Distillation, Pruning & 8T tokens from Nemotron-4 \\
    Orca/Orca 2 \cite{mitra2023orca, mukherjee2023orca} & 7B; 13B & Transformer & & \checkmark & Distillation & Orca 2 dataset, FLAN-v2 \cite{longpre2023flan} \\
    MINIMA \cite{zhang2023towards} & 3B & Transformer & & \checkmark & Distillation (from Llama-2-7B) & Pile \cite{gao2020pile}, Wudao \\
    Dolly-v2 \cite{DatabricksBlog2023DollyV2} & 3B; 7B & Transformer & & \checkmark & Instruction tuning (from Pythia) & Databricks-dolly-15k \\
    LaMini-LM \cite{wu-etal-2024-lamini} & 61M-7B & Transformer & & \checkmark & Distillation & LaMini instruction dataset \\
    \addlinespace[0.5ex]

    \midrule
    \multicolumn{7}{l}{\textit{I.C. Model Compression Approaches}} \\
    \addlinespace[0.5ex]
    SparseGPT \cite{frantar2023sparsegpt} & Various & Transformer & & \checkmark & Unstructured Pruning & Not applicable \\
    Wanda \cite{sun2024a} & Various & Transformer & & \checkmark & Unstructured Pruning & Not applicable \\
    LoRAPrune \cite{zhang2023loraprune} & Various & Transformer & & \checkmark & Unstructured Pruning & Not applicable \\
    ShortGPT \cite{men2024shortgpt} & Various & Transformer & & \checkmark & Structured Pruning & Not applicable \\
    BitNet/BitNet b1.58 \cite{wang2023bitnet, ma2024era} & Various & Transformer & & \checkmark & Quantization (QAT) & Not applicable \\
    QLoRA \cite{dettmers2024qlora} & Various & Transformer & & \checkmark & Quantization, Low-Rank & Various fine-tuning datasets \\
    SqueezeLLM \cite{kim2023squeezellm} & Various & Transformer & & \checkmark & Quantization (PTQ) & Not applicable \\
    \midrule
    
    \multicolumn{7}{l}{\textbf{\textit{II. Alternative Architecture Models}}} \\
    \midrule
    
    % \multicolumn{7}{l}{\textit{II.A. State Space Models and RNNs}} \\
    \addlinespace[0.5ex]
    Mamba \cite{gu2023mamba} & 125M-1.3B & Mamba & \checkmark & & Pre-training & Pile \cite{gao2020pile} \\
    Rene \cite{Rene} & 1.3B & Mamba & \checkmark & & Pre-training & Dolma-1.7 \cite{dolma} \\
    Zamba2 \cite{glorioso2024zambacompact7bssm} & 2.7B & Mamba & \checkmark & & Pre-training & Not specified \\
    Hymba \cite{dong2024hymba} & 125M-1.5B & Hymba & \checkmark & & Pre-training & DCLM-Baseline \cite{li2024datacomp} \\
    xLSTM \cite{beck2024xlstm} & 125M-1.3B & xLSTM & \checkmark & & Pre-training & SlimPajama \cite{cerebras2023slimpajama} \\
    RWKV \cite{peng-etal-2023-rwkv} & 169M-14B & RNN & \checkmark & & Pre-training & Pile \cite{gao2020pile} \\
    \addlinespace[0.5ex]
    
    % \multicolumn{7}{l}{\textit{II.B. Encoder-Decoder Models}} \\
    \addlinespace[0.5ex]
    Specialized FlanT5 \cite{fu2023specializing} & 250M-3B & Encoder-Decoder & & \checkmark & Instruction Tuning & GSM8K \cite{cobbe2021gsm8k} \\
    FlanT5 \cite{chung2024scaling} & 80M-3B & Encoder-Decoder & & \checkmark & Instruction Tuning & Muffin, T0-SF, SNI and CoT \\
    T5 \cite{raffel2020exploring} & 60M-3B & Encoder-Decoder & \checkmark & & Pre-training & C4 \cite{raffel2020exploring} \\
    \addlinespace[0.5ex]
    
    % \multicolumn{7}{l}{\textit{II.C. Encoder-Only Models}} \\
    \addlinespace[0.5ex]
    DistilBERT \cite{sanh2019distilbert} & 66M & Encoder-only & & \checkmark & Distillation (from BERT) & Wikipedia, BookCorpus \\
    TinyBERT \cite{jiao2020tinybert} & 14.5M & Encoder-only & & \checkmark & Distillation (from BERT) & Wikipedia, BookCorpus \\
    ALBERT \cite{lan2020albert} & 12M-18M & Encoder-only & \checkmark & & Pre-training (parameter sharing) & Wikipedia, BookCorpus \\
    \bottomrule
    \end{tabularx}
    \end{table}

    This classification showcases the architectural innovations and training methodologies that are driving the SLM field forward, providing essential technical foundations for deploying powerful AI capabilities on resource-constrained edge devices. By documenting various model sizes, training corpora, and development techniques, the table offers a comprehensive overview of cutting-edge approaches that enable sophisticated language processing directly on end-user devices. These advancements represent critical building blocks for the next generation of on-device AI systems that can operate efficiently without constant cloud connectivity while still delivering robust performance across diverse applications.



\section{Distributed Collaborative Frameworks}
\label{app:distributed_collaborative_frameworks}
Distributed collaborative frameworks enable the deployment, training, and fine-tuning of language models across multiple devices or servers. Table \ref{tab:framework_comparison} presents a comparison of prominent frameworks in this domain. These frameworks can be broadly categorized into three types: cloud-based platforms that offer centralized resources for distributed computing, federated learning systems that enable training across decentralized data sources while preserving privacy, and fully decentralized frameworks that distribute computation across peer nodes. Some frameworks like Neurosurgeon \cite{kang2017neurosurgeon}, MoE$^2$ \cite{jin2025moe2}, Edgent \cite{li2018edgent}, and Galaxy \cite{ye2024galaxy} focus on collaborative inference by partitioning models between edge devices and servers. Others, such as FedLLM \cite{wu2024fedllm}, FedFM \cite{chen2024fedfm}, FedPET \cite{li2024fedpet}, and Photon \cite{sani2024photon}, specialize in federated fine-tuning of large language models while maintaining data privacy. These frameworks are essential for enabling efficient deployment of language models in resource-constrained environments and for scenarios requiring privacy preservation or operation in disconnected settings.

\begin{table}[h!]
    \centering
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{l*{7}{c}}
    \toprule
    & \multicolumn{4}{c}{\textbf{Distributed Capabilities}} & & & \\
    \cmidrule(lr){2-5}
    \textbf{Framework} & \textbf{Inference} & \textbf{Training} & \textbf{Pretraining} & \textbf{Fine-tuning} & \textbf{Type} & \textbf{Privacy} & \textbf{License} \\
    \midrule
    exo-explore/exo \cite{exo} & \checkmark &  &  &  & Decentralized &  & MIT \\
    Together AI \cite{together} & \checkmark & \checkmark & \checkmark & \checkmark & Cloud &  & Commercial \\
    FLock Platform \cite{flock} &  & \checkmark &  & \checkmark & Federated, Blockchain & \checkmark & Apache 2.0  \\
    OpenDiloco \cite{OpenDiLoCo} &  & \checkmark & \checkmark &  & Decentralized &  & Apache 2.0 \\
    FederatedScope \cite{federatedscope} &  & \checkmark &  & \checkmark & Federated & \checkmark & Apache 2.0 \\
    FedML \cite{fedml} &  & \checkmark &  & \checkmark & Federated & \checkmark & Apache 2.0 \\
    Flower \cite{Flower} &  & \checkmark &  & \checkmark & Federated & \checkmark & Apache 2.0 \\
    FATE-LLM \cite{fan2023fate} &  & \checkmark &  & \checkmark & Federated & \checkmark & Apache 2.0 \\
    FedLLM \cite{ye2025fedllm} &  & \checkmark & \checkmark & \checkmark & Federated & \checkmark & CC BY-NC 4.0 \\
    \bottomrule
    \end{tabular}
    \caption{Comparison of Distributed Machine Learning Frameworks}
    \label{tab:framework_comparison}
\end{table}