% 核心问题：为什么要写这篇文章？主要观点是什么？
% 当前AI发展面临什么问题？（数据和算力瓶颈）
% 有什么潜在的解决方案？（分布式小型边缘设备）
% 这个解决方案带来什么机遇？（开启AI新纪元）

\begin{abstract}
The remarkable success of foundation models has been driven by scaling laws, demonstrating that model performance improves predictably with increased \textit{training data} and \textit{model size}. 
However, this scaling trajectory faces two critical challenges: the depletion of high-quality public data, 
% as evidenced by the exhaustion of web-scale text corpora,
and the prohibitive computational power required for larger models, 
which have been monopolized by tech giants. 
These two bottlenecks pose significant obstacles to the further development of AI.
In this position paper, we argue that leveraging massive distributed edge devices can break through these barriers. 
We reveal the vast untapped potential of data and computational resources on massive edge devices, and review recent technical advancements in distributed/federated learning that make this new paradigm viable.
Our analysis suggests that by collaborating on edge devices, everyone can participate in training large language models with small edge devices.
This paradigm shift towards distributed training on edge has the potential to democratize AI development and foster a more inclusive AI community.
\end{abstract}
