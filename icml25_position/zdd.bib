@article{lecun1998mnist,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={IEEE}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},
  pages={248--255},
  year={2009},
  organization={IEEE}
}

@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in Neural Information Processing Systems},
  volume={25},
  pages={1097--1105},
  year={2012}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{villalobos2022trends,
  title={Trends in training dataset sizes},
  author={Villalobos, Pablo and Ho, Anson},
  journal={Epoch AI Blog},
  year={2022},
  url={https://epochai.org/blog/trends-in-training-dataset-sizes}
}

@article{besiroglu2022projecting,
  title={Projecting compute trends in machine learning},
  author={Besiroglu, Tamay and Heim, Lennart and Sevilla, Jaime},
  journal={Epoch AI Blog},
  year={2022},
  url={https://epochai.org/blog/projecting-compute-trends}
}

@article{ho2023limits,
  title={Limits to the energy efficiency of CMOS microprocessors},
  author={Ho, Anson and Erdil, Ege and Besiroglu, Tamay},
  journal={2023 IEEE International Conference on Rebooting Computing (ICRC)},
  pages={1--10},
  year={2023},
  publisher={IEEE}
}

@article{sevilla2022compute,
  title={Compute trends across three eras of machine learning},
  author={Sevilla, Jaime and Heim, Lennart and Ho, Anson and Besiroglu, Tamay and Hobbhahn, Marius and Villalobos, Pablo},
  journal={2022 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--10},
  year={2022},
  publisher={IEEE}
}

@article{griffin2024chatgpt,
  title={ChatGPT creators OpenAI are generating 100 billion words per day, CEO says},
  author={Griffin, Andrew},
  journal={The Independent},
  year={2024},
  url={https://www.independent.co.uk/tech/chatgpt-openai-words-sam-altman-b2494900.html}
}

@article{yang2023leandojo,
  title={Leandojo: Theorem proving with retrieval-augmented language models},
  author={Yang, Kevin and Swope, Alex M and Gu, Albert and Chalamala, Ravi and Song, Peter and Yu, Stephen and Godil, Suhail and Prenger, Ryan and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2306.15626},
  year={2023}
}

@article{liu2023tinygsm,
  title={TinyGSM: Achieving 80\% on GSM8k with small models},
  author={Liu, Bingbin and Bubeck, S{\'e}bastien and Eldan, Ronen and Kulkarni, Janardhan and Li, Yuanzhi and Nguyen, Anh and Ward, Rachel and Zhang, Yi},
  journal={arXiv preprint arXiv:2312.09237},
  year={2023}
}

@article{shumailov2023curse,
  title={The curse of recursion: Training on generated data makes models forget},
  author={Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Gal, Yarin and Papernot, Nicolas and Anderson, Ross},
  journal={arXiv preprint arXiv:2305.17493},
  year={2023}
}

@article{singh2023beyond,
  title={Beyond human data: Scaling self-training for problem-solving with language models},
  author={Singh, Amanpreet and Co-Reyes, John D and Agarwal, Rishabh and Anand, Ankesh and Patil, Piyush and Garcia, Xavier and Liu, Peter J and Harrison, Josh and Lee, Jaehoon and Xu, Kelvin and others},
  journal={arXiv preprint arXiv:2312.06585},
  year={2023}
}

@article{alemohammad2023self,
  title={Self-consuming generative models go mad},
  author={Alemohammad, Sina and Casco-Rodriguez, Jose and Luzi, Lorenzo and Humayun, Ahmed Imtiaz and Babaei, Hossein and LeJeune, Daniel and Siahkoohi, Ali and Baraniuk, Richard G},
  journal={arXiv preprint arXiv:2307.01850},
  year={2023}
}

@article{fan2023scaling,
  title={Scaling laws of synthetic images for model training},
  author={Fan, Li and Chen, Kaiming and Krishnan, Dilip and Katabi, Dina and Isola, Phillip and Tian, Yuandong},
  journal={arXiv preprint arXiv:2306.09387},
  year={2023}
}

@article{villalobos2022trends,
  title={Trends in training dataset sizes},
  author={Villalobos, Pablo and Ho, Anson},
  journal={Epoch AI Blog},
  year={2022},
  url={https://epochai.org/blog/trends-in-training-dataset-sizes}
}

@article{rosenfeld2018whatsapp,
  title={WhatsApp usage patterns and prediction of demographic characteristics without access to message content},
  author={Rosenfeld, Avi and Sina, Shai and Sarne, David and Avidov, Oded and Kraus, Sarit},
  journal={Demographic Research},
  volume={39},
  pages={647--670},
  year={2018},
  doi={10.4054/DemRes.2018.39.22}
}

@article{konevcny2016federated,
  title={Federated learning: Strategies for improving communication efficiency},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Yu, Felix X and Richt{\'a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  journal={arXiv preprint arXiv:1610.05492},
  year={2016}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Sourabh and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{villalobos2024will,
  title={Will we run out of data? Limits of LLM scaling based on human-generated data},
  author={Villalobos, Pablo and Ho, Anson and Sevilla, Jaime and Besiroglu, Tamay and Heim, Lennart and Hobbhahn, Marius},
  journal={arXiv preprint arXiv:2211.04325},
  pages={13--29},
  year={2024}
}

@online{statista_iot_2023,
  author = {{Statista IoT device data volume}},
  title = {Internet of Things (IoT) connected devices data size worldwide from 2019 to 2025},
  year = {2023},
  url = {https://www.statista.com/statistics/1017863/worldwide-iot-connected-devices-data-size/},
  urldate = {2024-01-30},
  organization = {Statista Research Department}
}

@online{statista_global_2023,
  author = {{Statista global data volume}},
  title = {Volume of data/information created, captured, copied, and consumed worldwide from 2010 to 2025},
  year = {2023},
  url = {https://www.statista.com/statistics/871513/worldwide-data-created/},
  urldate = {2024-01-30},
  organization = {Statista Research Department}
}

@online{grandview_edge_2023,
  author = {{Grand View Research}},
  title = {Edge Computing Market Size \& Share Analysis Report, 2023-2030},
  year = {2023},
  url = {https://www.grandviewresearch.com/horizon/outlook/edge-computing-market-size/global},
  urldate = {2024-01-30},
  organization = {Grand View Research}
}

@online{bankmycell_smartphone_2023,
  author = {{BankMyCell}},
  title = {How Many Smartphones Are In The World?},
  year = {2023},
  url = {https://www.bankmycell.com/blog/how-many-phones-are-in-the-world},
  urldate = {2024-01-30},
  organization = {BankMyCell}
}


@online{counterpoint_smartphone_2021,
  author = {{CounterPoint}},
  title = {How Many Smartphones Are In The World?},
  year = {2021},
  url = {https://www.counterpointresearch.com/insights/smartphone-storage-capacity-zooms-increased-demand/},
  organization = {CounterPoint}
}


@online{grandviewresearch_edgecomputing,
  author = {{Grand View Research}},
  title = {Edge Computing Market Size, Share \& Forecast - Industry Insights},
  year = {2023},
  url = {https://www.grandviewresearch.com/industry-analysis/edge-computing-market},
}

@report{seagate_rethinkdata_2020,
  author = {Seagate},
  title = {Rethink Data Report 2020},
  year = {2020},
  url = {https://www.seagate.com/content/dam/seagate/migrated-assets/www-content/our-story/rethink-data/files/Rethink_Data_Report_2020.pdf},
}

@report{idc_seagate_dataage_2019,
  author = {Seagate},
  title = {DataAge White Paper: The Digitization of the World – From Edge to Core},
  year = {2019},
  url = {https://www.seagate.com/files/www-content/our-story/trends/files/idc-seagate-dataage-whitepaper.pdf},
}

@article{nayak2024review,
  title={A review on edge analytics: Issues, challenges, opportunities, promises, future directions, and applications},
  author={Nayak, Sabuzima and Patgiri, Ripon and Waikhom, Lilapati and Ahmed, Arif},
  journal={Digital Communications and Networks},
  volume={10},
  number={3},
  pages={783--804},
  year={2024},
  publisher={Elsevier}
}

@article{zhang2024survey,
  title={A survey of edge caching security: Framework, methods, and challenges},
  author={Zhang, Hang and Wang, Jinsong and Zhao, Zening and Zhao, Zhao},
  journal={Journal of Systems Architecture},
  pages={103306},
  year={2024},
  publisher={Elsevier}
}

@misc{cavliwireless_edgecomputing,
    author = {Cavli Wireless},
    title = {{Edge Computing for IoT, Real-Time Data and Low Latency Processing}},
    year = {2023},
    url = {https://www.cavliwireless.com/blog/nerdiest-of-things/edge-computing-for-iot-real-time-data-and-low-latency-processing.html},
    note = {Accessed:2025-01-22} % Replace with the date you accessed the content.
}

@article{Islam2024HyperPersonalizationWA,
  title={Hyper-Personalization with AI and Edge Computing: The Future of Customer Experience in E-commerce and Retail},
  author={Rashedul Islam and Md Sheam Arafat and Md Anwarul Matin Jony and Shahariar Rafi and Muhammad Saqib Jalil and Foysal Hossen},
  journal={Advanced International Journal of Multidisciplinary Research},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:274299739}
}

@misc{xenonstack_edge_ai_2023,
  author       = {XenonStack},
  title        = {Edge AI for Personalization},
  howpublished = {\url{https://www.xenonstack.com/blog/edge-ai-for-personalization}},
  year         = {2023},
note = {Accessed:2025-01-22} % Replace with the date you accessed the content.
}

@article{dohmatob2024strong,
  title={Strong model collapse},
  author={Dohmatob, Elvis and Feng, Yunzhen and Subramonian, Arjun and Kempe, Julia},
  journal={arXiv preprint arXiv:2410.04840},
  year={2024}
}