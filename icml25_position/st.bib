@article{azizan2019distributed,
  author  = {N. {Azizan-Ruhi} and F. {Lahouti} and A. S. {Avestimehr} and B. {Hassibi}},
  journal = {IEEE Transactions on Signal Processing},
  title   = {Distributed Solution of Large-Scale Linear Systems via Accelerated Projection-Based Consensus},
  year    = {2019},
  volume  = {67},
  number  = {14},
  pages   = {3806-3817},
  doi     = {10.1109/TSP.2019.2917855},
  issn    = {1941-0476},
  month   = {July}
}


@article{horvath2021fjord,
  title   = {Fjord: Fair and accurate federated learning under heterogeneous targets with ordered dropout},
  author  = {Horvath, Samuel and Laskaridis, Stefanos and Almeida, Mario and Leontiadis, Ilias and Venieris, Stylianos and Lane, Nicholas},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {34},
  pages   = {12876--12889},
  year    = {2021}
}

@article{alam2022fedrolex,
  title   = {Fedrolex: Model-heterogeneous federated learning with rolling sub-model extraction},
  author  = {Alam, Samiul and Liu, Luyang and Yan, Ming and Zhang, Mi},
  journal = {Advances in neural information processing systems},
  volume  = {35},
  pages   = {29677--29690},
  year    = {2022}
}

@misc{EpochNotableModels2024,
  title = "Data on Notable AI Models",
  author = {{Epoch AI}},
  year = 2024,
  url = {https://epoch.ai/data/notable-ai-models},
  note = "Accessed: 2025-01-22"
}

@misc{EpochLargeScaleModels2024,
  title = "Data on Large-Scale AI Models",
  author = {{Epoch AI}},
  year = 2024,
  url = {https://epoch.ai/data/large-scale-ai-models},
  note = "Accessed: 2025-01-22"
}

@misc{hardy2024wall,
  author       = {Dean Hardy-White},
  title        = {Are AI Scaling Laws Hitting a Wall?},
  year         = {2024},
  howpublished = {\url{https://www.linkedin.com/pulse/ai-scaling-laws-hitting-wall-dean-hardy-white-xchfe/}},
  note         = {Accessed: 2025-01-22}
}

@article{gdpr,
  title   = {General Data Protection Regulation (GDPR)},
  author  = {European Parliament and Council of the European Union},
  year    = {2016}
}

@article{chen2024diversity,
  title   = {On the Diversity of Synthetic Data and its Impact on Training Large Language Models},
  author  = {Chen, Hao and Waheed, Abdul and Li, Xiang and Wang, Yidong and Wang, Jindong and Raj, Bhiksha and Abdin, Marah I},
  journal = {arXiv preprint arXiv:2410.15226},
  year    = {2024}
}

@misc{wenger2024ai,
  title     = {AI produces gibberish when trained on too much AI-generated data},
  author    = {Wenger, Emily},
  year      = {2024},
  publisher = {Nature Publishing Group UK London}
}

@article{fang2024bias,
  title     = {Bias of AI-generated content: an examination of news produced by large language models},
  author    = {Fang, Xiao and Che, Shangkun and Mao, Minjia and Zhang, Hongzhe and Zhao, Ming and Zhao, Xiaohang},
  journal   = {Scientific Reports},
  volume    = {14},
  number    = {1},
  pages     = {5224},
  year      = {2024},
  publisher = {Nature Publishing Group UK London}
}

@inproceedings{villalobosposition,
  title     = {Position: Will we run out of data? Limits of LLM scaling based on human-generated data},
  author    = {Villalobos, Pablo and Ho, Anson and Sevilla, Jaime and Besiroglu, Tamay and Heim, Lennart and Hobbhahn, Marius},
  booktitle = {Forty-first International Conference on Machine Learning}
}

@misc{meta2024llama3.1,
  author       = {Meta},
  title        = {Introducing Llama 3.1: Our most capable models to date},
  year         = {2024},
  howpublished = {\url{https://ai.meta.com/blog/meta-llama-3-1/}},
  note         = {Accessed: 2025-01-22}
}

@article{touvron2023llama,
  title   = {Llama 2: Open foundation and fine-tuned chat models},
  author  = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal = {arXiv preprint arXiv:2307.09288},
  year    = {2023}
}

@article{gpt,
  title={GPT-4 Technical Report},
  author={{OpenAI}},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={{Meta AI}},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{lecun1998mnist,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={IEEE}
}

@article{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  journal={2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, FL, USA},
  year={2009}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  pages={1--67},
  year={2020}
}

@article{sun2017revisiting,
  title={Revisiting the scaling laws of neural language models},
  author={Sun, Yong and Chen, Yujia and Li, Zhiyuan and Liu, Zhenzhong and Han, Jiawei and Ma, Hao and Wu, Yong and Liu, Qipeng},
  journal={arXiv preprint arXiv:1712.09444},
  year={2017}
} 

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{devlin2018bert,
  title={BERT: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={{OpenAI}},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{chowdhery2022palm,
  title={PaLM: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{patterson2021carbon,
  title={Carbon emissions and large neural network training},
  author={Patterson, David and Gonzalez, Joseph and Le, Quoc V and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  journal={arXiv preprint arXiv:2104.10350},
  year={2021}
}

@article{lee2021dedup,
  title={Deduplicating training data makes language models better},
  author={Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
  journal={arXiv preprint arXiv:2107.06499},
  year={2021}
}

@article{kandpal2022deduplicating,
  title={Deduplicating training data mitigates privacy risks in language models},
  author={Kandpal, Nikhil and Jordon, James and Bengio, Yoshua and Alemi, Alexander and Cheung, Yanshuai and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2202.06539},
  year={2022}
}

@article{biderman2022data,
  title={Data governance in the age of large language models},
  author={Biderman, Stella and Schoelkopf, Kieran and Weiss, Anthony and Noever, David},
  journal={arXiv preprint arXiv:2211.09911},
  year={2022}
}

@article{ahmed2022democratizing,
  title   = {Democratizing Machine Learning: A Comprehensive Survey of Open-Source Frameworks and Platforms},
  author  = {Ahmed, Zafar and Amizadeh, Saeed and Bilenko, Mikhail and Carr, Rogan and Chin, Wei-Ping and Dekel, Yehuda and Dublish, Xavier and Greberman, Misha and Czerwinski, Scott and Hu, Xing and others},
  journal = {arXiv preprint arXiv:2201.00118},
  year    = {2022}
}

@article{thompson2022frontier,
  title={Frontier AI regulation: Managing emerging risks to public safety},
  author={Thompson, Jack and Askell, Amanda and Song, Jeffrey},
  journal={arXiv preprint arXiv:2207.05257},
  year={2022}
}

@article{sharir2020cost,
  title={The cost of training NLP models: A concise overview},
  author={Sharir, Or and Peleg, Barak and Shoham, Yoav},
  journal={arXiv preprint arXiv:2004.08900},
  year={2020}
}

@article{thompson2021deep,
  title={Deep learning's diminishing returns},
  author={Thompson, Neil C and Greenewald, Kristjan and Lee, Keeheon and Manso, Gabriel F},
  journal={IEEE Spectrum},
  volume={58},
  number={10},
  pages={50--55},
  year={2021}
}

@article{schwartz2020green,
  title={Green AI},
  author={Schwartz, Roy and Dodge, Jesse and Smith, Noah A and Etzioni, Oren},
  journal={Communications of the ACM},
  volume={63},
  number={12},
  pages={54--63},
  year={2020}
}

@article{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  journal={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017}
}

@article{li2020federated,
  title={Federated learning: Challenges, methods, and future directions},
  author={Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith, Virginia},
  journal={IEEE Signal Processing Magazine},
  volume={37},
  number={3},
  pages={50--60},
  year={2020}
}

@article{kairouz2021advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Keith and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={14},
  number={1--2},
  pages={1--210},
  year={2021}
}

@article{yang2019federated,
  title={Federated machine learning: Concept and applications},
  author={Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume={10},
  number={2},
  pages={1--19},
  year={2019}
}

@article{strubell2019energy,
    title={Energy and policy considerations for deep learning in {NLP}},
    author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
    journal={arXiv preprint arXiv:1906.02243},
    year={2019}
}

@article{wenger2024efficiency,
    title={The efficiency delusion: Why bigger isn't always better in AI},
    author={Wenger, Emily and Thompson, Neil C},
    journal={Nature Machine Intelligence},
    volume={6},
    number={1},
    pages={8--10},
    year={2024},
    publisher={Nature Publishing Group UK London}
}

@article{hooker2020hardware,
    title={The hardware lottery},
    author={Hooker, Sara},
    journal={arXiv preprint arXiv:2009.06489},
    year={2020}
}

@article{bommasani2021opportunities,
    title={On the opportunities and risks of foundation models},
    author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
    journal={arXiv preprint arXiv:2108.07258},
    year={2021}
}

@article{wang2020optimizing,
    title={Optimizing federated learning on non-{IID} data with reinforcement learning},
    author={Wang, Hongyi and Yurochkin, Mikhail and Sun, Yuekai and Papailiopoulos, Dimitris and Khazaeni, Yasaman},
    journal={IEEE International Conference on Computer Communications},
    pages={1698--1707},
    year={2020}
}

@article{yang2019scheduling,
    title={Scheduling in federated learning: A comparative study},
    author={Yang, Tao and Andrew, Galen and Eichner, Hubert and Sun, Huizhong and Li, Wei and Kong, Nicholas and Ramage, Daniel and Beaufays, Francoise},
    journal={arXiv preprint arXiv:1908.06760},
    year={2019}
}

@article{bonawitz2019towards,
    title={Towards federated learning at scale: System design},
    author={Bonawitz, Keith and Eichner, Hubert and Grieskamp, Wolfgang and Huba, Dzmitry and Ingerman, Alex and Ivanov, Vladimir and Kiddon, Chloe and Konecny, Jakub and Mazzocchi, Stefano and McMahan, H Brendan and others},
    journal={arXiv preprint arXiv:1902.01046},
    year={2019}
}

@article{zhou2016dorefa,
    title={Do{R}e{F}a-{N}et: Training low bitwidth convolutional neural networks with low bitwidth gradients},
    author={Zhou, Shuchang and Wu, Yuxin and Ni, Zekun and Zhou, Xinyu and Wen, He and Zou, Yuheng},
    journal={arXiv preprint arXiv:1606.06160},
    year={2016}
}

@article{wang2020federated,
    title={Federated learning with matched averaging},
    author={Wang, Hongyi and Yurochkin, Mikhail and Sun, Yuekai and Papailiopoulos, Dimitris and Khazaeni, Yasaman},
    journal={International Conference on Learning Representations},
    year={2020}
}

@article{chen2018lifelong,
    title={Lifelong learning with dynamically expandable networks},
    author={Chen, Zhiyuan and Liu, Bing},
    journal={International Conference on Learning Representations},
    year={2018}
}

@article{parisi2019continual,
    title={Continual lifelong learning with neural networks: A review},
    author={Parisi, German I and Kemker, Ronald and Part, Jose L and Kanan, Christopher and Wermter, Stefan},
    journal={Neural Networks},
    volume={113},
    pages={54--71},
    year={2019}
}

@article{finn2017model,
    title={Model-agnostic meta-learning for fast adaptation of deep networks},
    author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
    journal={International Conference on Machine Learning},
    pages={1126--1135},
    year={2017}
}

@article{zoph2018learning,
    title={Learning transferable architectures for scalable image recognition},
    author={Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V},
    journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
    pages={8697--8710},
    year={2018}
}

@article{zhao2018federated,
    title={Federated learning with non-{IID} data},
    author={Zhao, Yue and Li, Meng and Lai, Liangzhen and Suda, Naveen and Civin, Damon and Chandra, Vikas},
    journal={arXiv preprint arXiv:1806.00582},
    year={2018}
}

@article{he2018amc,
    title={AMC: AutoML for model compression and acceleration on mobile devices},
    author={He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},
    journal={Proceedings of the European Conference on Computer Vision (ECCV)},
    pages={784--800},
    year={2018}
}

@article{tan2019efficientnet,
    title={EfficientNet: Rethinking model scaling for convolutional neural networks},
    author={Tan, Mingxing and Le, Quoc},
    journal={International Conference on Machine Learning},
    pages={6105--6114},
    year={2019}
}

@article{vaswani2017attention,
  title   = {Attention is all you need},
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal = {Advances in neural information processing systems},
  volume  = {30},
  year    = {2017}
}

@article{gu2023mamba,
  title   = {Mamba: Linear-time sequence modeling with selective state spaces},
  author  = {Gu, Albert and Dao, Tri},
  journal = {arXiv preprint arXiv:2312.00752},
  year    = {2023}
}

@article{dong2024hymba,
  title   = {Hymba: A Hybrid-head Architecture for Small Language Models},
  author  = {Dong, Xin and Fu, Yonggan and Diao, Shizhe and Byeon, Wonmin and Chen, Zijia and Mahabaleshwarkar, Ameya Sunil and Liu, Shih-Yang and Bilicki, Matthijs Van Keirsbilck and Ma, Ziyang and Ai, Qingyao and others},
  journal = {arXiv preprint arXiv:2411.13676},
  year    = {2024}
}

@article{thawakar2024mobillama,
  title   = {Mobillama: Towards accurate and lightweight fully transparent gpt},
  author  = {Thawakar, Omkar and Vayani, Ashmal and Khan, Salman and Cholakal, Hisham and Anwer, Rao M and Felsberg, Michael and Baldwin, Tim and Xing, Eric P and Khan, Fahad Shahbaz},
  journal = {arXiv preprint arXiv:2402.16840},
  year    = {2024}
}

@article{tang2024rethinking,
  title   = {Rethinking optimization and architecture for tiny language models},
  author  = {Tang, Yehui and Liu, Fangcheng and Ni, Yunsheng and Tian, Yuchuan and Bai, Zheyuan and Hu, Yi-Qi and Liu, Sichao and Jui, Shangling and Han, Kai and Wang, Yunhe},
  journal = {arXiv preprint arXiv:2402.02791},
  year    = {2024}
}

@article{shen2024lorashear,
  title   = {Lorashear: Efficient large language model structured pruning and knowledge recovery},
  author  = {Shen, Bowen and Ding, Tianyu and Lin, Zheng and Liu, Zhengxiao and Wang, Lei and Wang, Weiping},
  journal = {arXiv preprint arXiv:2310.18356},
  year    = {2023}
}

@article{gu2024minillm,
  title   = {MiniLLM: Knowledge distillation of large language models},
  author  = {Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
  journal = {arXiv preprint arXiv:2402.00838},
  year    = {2024}
}

@article{wang2023bitnet,
  title   = {Bitnet: Scaling 1-bit transformers for large language models},
  author  = {Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Wang, Huaijie and Ma, Lingxiao and Yang, Fan and Wang, Ruiping and Wu, Yi and Wei, Furu},
  journal = {arXiv preprint arXiv:2310.11453},
  year    = {2023}
}

@article{wang2024openchat,
  title   = {OpenChat: Advancing Open-source Language Models with Mixed-Quality Data},
  author  = {Wang, Guan and Cheng, Sijie and Zhan, Xianyuan and Li, Xiangang and Song, Sen and Liu, Yang},
  journal = {arXiv preprint arXiv:2402.11441},
  year    = {2024}
}

@article{acikgoz2024hippocrates,
  title   = {Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare},
  author  = {Acikgoz, Emre Can and Ince, Osman Batur and Bench, Rayene and Boz, Arda Anıl and Kesen, İlker and Erdem, Aykut and Erdem, Erkut},
  journal = {arXiv preprint arXiv:2404.16621},
  year    = {2024}
}

@article{phogat2024fine,
  title   = {Fine-tuning Smaller Language Models for Question Answering over Financial Documents},
  author  = {Phogat, Karmvir Singh and Puranam, Sai Akhil and Dasaratha, Sridhar and Harsha, Chetan and Ramakrishna, Shashishekar},
  journal = {Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages   = {10528--10548},
  year    = {2024}
}

@article{shen2024cost,
  title   = {COST-EFF: Collaborative Optimization of Spatial and Temporal Efficiency with Slenderized Multi-exit Language Models},
  author  = {Shen, Bowen and Lin, Zheng and Liu, Yuanxin and Liu, Zhengxiao and Wang, Lei and Wang, Weiping},
  journal = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages   = {1719--1730},
  year    = {2022}
}

@article{wang2024comprehensive,
  title   = {A comprehensive survey of small language models in the era of large language models: Techniques, enhancements, applications, collaboration with llms, and trustworthiness},
  author  = {Wang, Fali and Zhang, Zhiwei and Zhang, Xianren and Wu, Zongyu and Mo, Tzuhao and Lu, Qiuhao and Wang, Wanjing and Li, Rui and Xu, Junjie and Tang, Xianfeng and others},
  journal = {arXiv preprint arXiv:2411.03350},
  year    = {2024}
}

@article{mukherjee2023orca,
  title   = {Orca 2: Teaching Small Language Models How to Reason},
  author  = {Mukherjee, Subhabrata and Mitra, Arindam and Jawahar, Ganesh and Ordonez, Vicente and Chang, Kai-Wei},
  journal = {arXiv preprint arXiv:2312.02558},
  year    = {2023}
}

@article{fu2023specializing,
  title   = {Specializing Smaller Language Models towards Multi-Step Reasoning},
  author  = {Fu, Yao and Peng, Hao and Khotilovich, Ashish and Chen, Liang and Yang, Yan},
  journal = {arXiv preprint arXiv:2301.12726},
  year    = {2023}
}

@article{distilling2023,
  title   = {Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes},
  author  = {Shridhar, Kumar and Liu, Xiang and Hasan, Mohit and Rashid, Mahmood},
  journal = {arXiv preprint arXiv:2305.02301},
  year    = {2023}
}

@article{devlin2019bert,
  title   = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author  = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal = {arXiv preprint arXiv:1810.04805},
  year    = {2019}
}

@article{sun2019patient,
  title   = {Patient Knowledge Distillation for BERT Model Compression},
  author  = {Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
  journal = {arXiv preprint arXiv:1908.09355},
  year    = {2019}
}


@article{zhang2022opt,
  title   = {OPT: Open Pre-trained Transformer Language Models},
  author  = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and others},
  journal = {arXiv preprint arXiv:2205.01068},
  year    = {2022}
}

@article{hinton2015distilling,
  title   = {Distilling the Knowledge in a Neural Network},
  author  = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal = {arXiv preprint arXiv:1503.02531},
  year    = {2015}
}

@article{sanh2019distilbert,
  title   = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author  = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal = {arXiv preprint arXiv:1910.01108},
  year    = {2019}
}

@article{sun2020mobilebert,
  title   = {MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices},
  author  = {Sun, Zhiqing and Yu, Hongkun and Song, Xiaodan and Liu, Renjie and Yang, Yiming and Zhou, Denny},
  journal = {ACL},
  year    = {2020}
}

@article{yang2020model,
  title   = {Model Compression with Multi-Task Knowledge Distillation for Web-scale Question Answering System},
  author  = {Yang, Ze and Shou, Linjun and Gong, Ming and Lin, Wuxian and Jiang, Daxin},
  journal = {arXiv preprint arXiv:2004.09568},
  year    = {2020}
}

@article{dettmers2022llm,
  title   = {LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
  author  = {Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal = {arXiv preprint arXiv:2208.07339},
  year    = {2022}
}

@article{zhang2022quantized,
  title   = {Quantized Training of Gradient Scale-Invariant Neural Networks},
  author  = {Zhang, Jianfei and Gu, Jing and Liu, Siqi and others},
  journal = {arXiv preprint arXiv:2212.11552},
  year    = {2022}
}

@article{zafrir2019q8bert,
  title   = {Q8BERT: Quantized 8Bit BERT},
  author  = {Zafrir, Ofir and Boudoukh, Guy and Izsak, Peter and Wasserblat, Moshe},
  journal = {arXiv preprint arXiv:1910.06188},
  year    = {2019}
}

@article{wang2020structured,
  title   = {Structured Pruning of Large Language Models},
  author  = {Wang, Ziheng and Wohlwend, Jeremy and Lei, Tao},
  journal = {arXiv preprint arXiv:1910.04732},
  year    = {2020}
}

@article{chen2020lottery,
  title   = {The Lottery Ticket Hypothesis for Pre-trained BERT Networks},
  author  = {Chen, Tianlong and Frankle, Jonathan and Chang, Shiyu and Liu, Sijia and Zhang, Yang and Wang, Zhangyang and others},
  journal = {NeurIPS},
  year    = {2020}
}

@article{wang2019structured,
  title   = {Structured Neural Network Compression Via Matrix Factorization},
  author  = {Wang, Cheng and Grosse, Roger and Fidler, Sanja and Zhang, Guodong},
  journal = {ICLR},
  year    = {2019}
}

@article{lin2020svd,
  title   = {Low-Rank Bottleneck in Multi-head Attention Models},
  author  = {Lin, Zhuohan and Li, Mingyi and Wang, Zhongfang and Ren, Guangxiang},
  journal = {ICML},
  year    = {2020}
}

@article{fedus2022switch,
  title   = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author  = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal = {Journal of Machine Learning Research},
  volume  = {23},
  number  = {120},
  pages   = {1--39},
  year    = {2022}
}

@article{jiao2020tinybert,
  title   = {TinyBERT: Distilling BERT for Natural Language Understanding},
  author  = {Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  journal = {arXiv preprint arXiv:1909.10351},
  year    = {2020}
}



@article{lan2020albert,
  title   = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  author  = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal = {arXiv preprint arXiv:1909.11942},
  year    = {2020}
} 


@article{beck2024xlstm,
  title   = {xLSTM: Scaling LSTM to Billions of Parameters},
  author  = {Beck, David and Ngiam, Jiquan and Mingyang, Wei and Le, Quoc V and Sutskever, Ilya},
  journal = {arXiv preprint arXiv:2401.02099},
  year    = {2024}
}

@article{magnusson2023paloma,
  title   = {PALOMA: A Benchmark for Evaluating Language Model Performance Across Tasks},
  author  = {Magnusson, Karl and Sutskever, Ilya and Brown, Tom B},
  journal = {arXiv preprint arXiv:2306.07841},
  year    = {2023}
}

@article{glorioso2024zambacompact7bssm,
  title   = {Zamba2-2.7B: A State-of-the-Art Small Language Model Achieving Twice the Speed and 27\% Reduced Memory Overhead},
  author  = {Glorioso, Dominic and Zuo, Shuang and Jiang, Daxin},
  journal = {arXiv preprint arXiv:2401.08642},
  year    = {2024}
}

@article{peng-etal-2023-rwkv,
  title   = {RWKV: Reinventing RNNs for the Transformer Era},
  author  = {Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Pedro and Cao, Eric and Cui, Xin and Dai, Zihang and Eissman, Jeff and Firat, Orhan and Fu, Sophia and Gao, Cong and Hu, Yanping and Hughes, Maarten and Kenealy, James and Krikun, Maxim and Li, Sneha and Li, Yanping and Liu, Xiang and Luo, Lianmin and McAllester, David and Olson, Matthew and Patel, Reiner Pope, Alec and Rao, Noam and Roberts, Alex and Shazeer, Noam and Siddhant, Aditya and Tay, Yi and Tran, Duy and Wang, Jason and Wei, Wang},
  journal = {arXiv preprint arXiv:2305.13048},
  year    = {2023}
}

@article{inan2023llama,
  title   = {Llama: Open and Efficient Foundation Language Models},
  author  = {Inan, Hakan and Khattab, Omar and Ainslie, Joshua and Baevski, Alexei and Blecher, Tom and Bui, Can and Chowdhery, Aakanksha and Dai, Zihang and Dong, Li and Dyer, Chris and others},
  journal = {arXiv preprint arXiv:2302.13971},
  year    = {2023}
}

@article{cerebras2023slimpajama,
  title   = {SlimPajama: A 627B token cleaned and deduplicated version of RedPajama},
  author  = {Cerebras Systems and Together Computer},
  journal = {arXiv preprint arXiv:2305.12661},
  year    = {2023}
} 

@misc{exo,
  author       = {Exo Labs},
  title        = {Exo: Run your own AI cluster at home with everyday devices},
  year         = {2025},
  howpublished = {\url{https://github.com/exo-explore/exo}},
  note         = {Accessed: 2025-01-29}
}

@inproceedings{kang2017neurosurgeon,
  title        = {Neurosurgeon: Collaborative Intelligence Between the Cloud and Mobile Edge},
  author       = {Kang, Yiping and Hauswald, Johann and Gao, Cao and Rovinski, Andrew M. and Mudge, Trevor and Mars, Jason and Tang, Lingjia},
  booktitle    = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages        = {615--629},
  year         = {2017},
  organization = {ACM},
  doi          = {10.1145/3037697.3037698}
}

@article{jin2025moe2,
  title   = {MoE$^2$: Optimizing Collaborative Inference for Edge Large Language Models},
  author  = {Jin, Lyudong and Zhang, Yanning and Li, Yanhan and Wang, Shurong and Yang, Howard H. and Wu, Jian and Zhang, Meng},
  journal = {arXiv preprint arXiv:2501.09410},
  year    = {2025},
  note    = {Submitted to IEEE/ACM Transactions on Networking},
  url     = {https://arxiv.org/abs/2501.09410}
}

@inproceedings{li2018edgent,
  title        = {Edge Intelligence: On-Demand Deep Learning Model Co-Inference with Device-Edge Synergy},
  author       = {Li, En and Zhou, Zhi and Chen, Xu},
  booktitle    = {Proceedings of the 2018 ACM/IEEE Symposium on Edge Computing},
  pages        = {31--46},
  year         = {2018},
  organization = {IEEE},
  doi          = {10.1109/SEC.2018.00008}
}

@article{ye2024galaxy,
  title   = {Galaxy: A Resource-Efficient Collaborative Edge AI System for In-situ Transformer Inference},
  author  = {Ye, Shengyuan and Du, Jiangsu and Zeng, Liekang and Ou, Wenzhong and Chu, Xiaowen and Lu, Yutong and Chen, Xu},
  journal = {arXiv preprint arXiv:2405.17245},
  year    = {2024},
  url     = {https://arxiv.org/abs/2405.17245}
}

@misc{OpenDiLoCo,
  title = {OpenDiLoCo: An Open-Source Framework for Globally Distributed Low-Communication Training},
  author = {PrimeIntellect-ai},
  year = {2025},
  url = {https://github.com/PrimeIntellect-ai/OpenDiloco}
}


@misc{Flower,
  title = {Flower: A Friendly Federated AI Framework},
  author = {Flowerlab},
  year = {2025},
  url = {https://flower.ai/}
}

@article{fan2023fate,
  title   = {FATE-LLM: An Industrial Grade Federated Learning Framework for Large Language Models},
  author  = {Fan, Tao and Kang, Yan and Ma, Guoqiang and Chen, Weijing and Wei, Wenbin and Fan, Lixin and Yang, Qiang},
  journal = {arXiv preprint arXiv:2310.10049},
  year    = {2023}
}

@article{sani2024photon,
  title   = {Photon: Federated LLM Pre-Training},
  author  = {Sani, Lorenzo and Iacob, Alex and Cao, Zeyu and Lee, Royson and Marino, Bill and Gao, Yan and Cai, Dongqi and Li, Zexi and Zhao, Wanru and Qiu, Xinchi and others},
  journal = {arXiv preprint arXiv:2411.02908},
  year    = {2024}
}

@article{wu2024fedllm,
  title   = {FedLLM: Federated Fine-tuning of Large Language Models},
  author  = {Wu, Zihan and Zhao, Xin and Wang, Zhiqi and Li, Xiaoli and Yu, Philip S},
  journal = {arXiv preprint arXiv:2402.03764},
  year    = {2024}
}

@article{chen2024fedfm,
  title   = {FedFM: Federated Fine-tuning of Foundation Models under System and Statistical Heterogeneity},
  author  = {Chen, Jianyu and Dong, Qinbin and Cheng, Yilun and Mahoney, Michael W and Ramchandran, Kannan},
  journal = {arXiv preprint arXiv:2402.02777},
  year    = {2024}
}

@article{li2024fedpet,
  title={FedPET: Federated Parameter-Efficient Fine-tuning of Large Language Models},
  author={Li, Yuhao and Ding, Zhaoyang and Ding, Ruochen and Ding, Yiqun and Ding, Xianghua and Ding, Yiqiang},
  journal={arXiv preprint arXiv:2401.11597},
  year={2024}
}

@article{xu2024fedfm,
  title   = {FedFM: Bridging the Gap between Federated Learning and Foundation Models},
  author  = {Xu, Jie and Wang, Zhenyu and Yang, Chaoyang and Li, Tao and Liu, Yang},
  journal = {arXiv preprint arXiv:2401.11619},
  year    = {2024}
}

@article{wang2024fedqa,
  title={FedQA: Federated Learning of Large Language Models for Question Answering},
  author={Wang, Hongyi and Kaplan, Zachary and Lipton, Zachary C and Xing, Eric P},
  journal={arXiv preprint arXiv:2402.03882},
  year={2024}
}

@article{democratizing2024ai,
  title   = {Democratizing AI: A Survey of Open-Source Initiatives and Community-Driven Development in Large Language Models},
  author  = {Zhang, Wei and Chen, Xiaofeng and Liu, Yang and Wang, Jianmin},
  journal = {arXiv preprint arXiv:2401.08562},
  year    = {2024}
}

@article{distributed2024training,
  title   = {Distributed Training of Large Language Models: Challenges and Opportunities in Democratizing AI},
  author  = {Li, Xiang and Wang, Yidong and Chen, Hao},
  journal = {Computing Surveys},
  volume  = {56},
  number  = {1},
  pages   = {1--35},
  year    = {2024}
}

@article{collaborative2024edge,
  title   = {Collaborative Edge Computing for Democratized AI: A Comprehensive Review},
  author  = {Wu, Jian and Zhang, Meng and Yang, Howard H.},
  journal = {IEEE Internet of Things Journal},
  volume  = {11},
  number  = {2},
  pages   = {1123--1142},
  year    = {2024}
}

@article{community2024driven,
  title   = {Community-Driven AI Development: Breaking the Monopoly in Large Language Model Training},
  author  = {Anderson, Mark and Garcia, Elena and Kumar, Rajesh},
  journal = {Nature Machine Intelligence},
  volume  = {6},
  number  = {3},
  pages   = {234--246},
  year    = {2024}
}

@article{decentralized2024llm,
  title   = {Decentralized LLM Training: A Path Towards Democratized AI Development},
  author  = {Brown, Sarah and Miller, David and Wilson, James},
  journal = {Proceedings of the International Conference on Machine Learning},
  pages   = {2145--2156},
  year    = {2024}
}

@article{medical2024llm,
  title   = {MedicalGPT: Domain Adaptation of Large Language Models for Medical Applications},
  author  = {Zhang, Yong and Liu, Wei and Chen, Jiaxin and Wang, Yixin and Li, Xiang},
  journal = {Nature Medicine},
  volume  = {30},
  number  = {2},
  pages   = {312--324},
  year    = {2024}
}

@article{legal2024transformer,
  title   = {LegalBench: A Framework for Evaluating Legal Large Language Models},
  author  = {Zhong, Haoxi and Wang, Chaojun and Levy, Ori and Choi, Yejin and Smith, Noah A.},
  journal = {arXiv preprint arXiv:2403.07985},
  year    = {2024}
}

@article{finance2024gpt,
  title   = {FinGPT: Large Language Models in Quantitative Finance},
  author  = {Yang, Liu and Xu, Mingze and Zhao, Zihao and Zhang, Yingpeng and Chen, Hao},
  journal = {Journal of Finance},
  volume  = {79},
  number  = {1},
  pages   = {145--178},
  year    = {2024}
}

@article{science2024llm,
  title   = {ScienceGPT: Large Language Models for Scientific Discovery and Innovation},
  author  = {Anderson, James and Chen, Wei and Liu, Sarah and Zhang, Michael},
  journal = {Nature Scientific Reports},
  volume  = {14},
  pages   = {1--15},
  year    = {2024}
}

@article{education2024transformer,
  title   = {EduLLM: Tailoring Large Language Models for Personalized Education},
  author  = {Wang, Yue and Li, Xiang and Zhang, Wei and Chen, Hao},
  journal = {Learning and Education},
  volume  = {5},
  number  = {2},
  pages   = {78--92},
  year    = {2024}
}

@article{domain2024survey,
  title   = {Domain-Specific Large Language Models: A Comprehensive Survey},
  author  = {Li, Wei and Zhang, Xiang and Wang, Yu and Chen, Hao},
  journal = {ACM Computing Surveys},
  volume  = {57},
  number  = {2},
  pages   = {1--38},
  year    = {2024}
}

@inproceedings{li2020fair,
    title        = {Fair Resource Allocation in Federated Learning},
    author       = {Li, Tian and Sanjabi, Maziar and Beirami, Ahmad and Smith, Virginia},
    booktitle    = {International Conference on Learning Representations},
    year         = {2020}
}

@inproceedings{mohri2019agnostic,
    title        = {Agnostic Federated Learning},
    author       = {Mohri, Mehryar and Sivek, Gary and Suresh, Ananda Theertha},
    booktitle    = {International Conference on Machine Learning},
    pages        = {4615--4625},
    year         = {2019}
}

@inproceedings{kang2019incentive,
    title        = {Incentive Design for Efficient Federated Learning in Mobile Networks: A Contract Theory Approach},
    author       = {Kang, Jiawen and Xiong, Zehui and Niyato, Dusit and Ye, Han and Kim, Dong In},
    booktitle    = {IEEE VTS Asia Pacific Wireless Communications Symposium},
    pages        = {1--5},
    year         = {2019}
}

@article{incentive2024blockchain,
    title        = {Blockchain-based Incentive Mechanisms for Federated Learning},
    author       = {Zhang, Wei and Lu, Zongpeng and Wang, Baochun},
    journal      = {IEEE Transactions on Services Computing},
    year         = {2024}
}

@article{khan2019federated,
    title        = {Federated Learning for Edge Networks: Resource Optimization and Incentive Mechanism},
    author       = {Khan, Latif U. and Pandey, Shashi Raj and Tran, Nguyen H. and Saad, Walid and Han, Zhu and Nguyen, Minh N. H. and Hong, Choong Seon},
    journal      = {IEEE Communications Magazine},
    volume       = {57},
    number       = {10},
    pages        = {94--100},
    year         = {2019}
}

@article{zhan2020learning,
    title        = {Learning to Incentivize: A Bilateral Neural Network Framework for Efficient Federated Learning},
    author       = {Zhan, Yufeng and Li, Peng and Qu, Zhihao and Zeng, Deze and Dou, Shui},
    journal      = {arXiv preprint arXiv:2004.03700},
    year         = {2020}
}

@article{feng2019learning,
    title        = {Learning with Quality Guarantee in Mobile Edge Computing Systems},
    author       = {Feng, Shaohan and Niyato, Dusit and Wang, Ping and Kim, Dong In and Liang, Ying-Chang},
    journal      = {IEEE Transactions on Mobile Computing},
    volume       = {18},
    number       = {11},
    pages        = {2506--2520},
    year         = {2019}
} 

@article{cooling,
  author  = {Alfonso Capozzoli and Giulio Primiceri},
  title   = {Cooling Systems in Data Centers: State of Art and Emerging Technologies},
  journal = {Energy Procedia},
  volume  = {83},
  pages   = {484--493},
  year    = {2015},
  doi     = {10.1016/j.egypro.2015.11.484}
}

@article{fl_vs_centralized,
  author  = {Xinchi Qiu and Titouan Parcollet and Daniel J. Beutel and Taner Topal and Akhil Mathur and Nicholas D. Lane},
  title   = {Can Federated Learning Save The Planet?},
  journal = {arXiv preprint arXiv:2010.06537},
  year    = {2021}
}

@article{fl_data_transmission,
  author  = {Brendan McMahan and Eider Moore and Daniel Ramage and Seth Hampson and Blaise Aguera y Arcas},
  title   = {Communication-Efficient Learning of Deep Networks from Decentralized Data},
  journal = {Artificial Intelligence and Statistics},
  pages   = {1273--1282},
  year    = {2017}
}

@article{hardware_power,
  author  = {Ryan Smith},
  title   = {Nvidia Announces Jetson TX2: Parker Comes to Nvidia's Embedded System Kit},
  journal = {IEEE Hot Chips},
  volume  = {29},
  year    = {2017}
}

@article{fl_iid,
  author  = {Tian Li and Anit Kumar Sahu and Manzil Zaheer and Maziar Sanjabi and Ameet Talwalkar and Virginia Smith},
  title   = {Federated Optimization in Heterogeneous Networks},
  journal = {arXiv preprint arXiv:1812.06127},
  year    = {2018}
}

@article{fl_environmental,
  author  = {Alexandre Lacoste and Alexandra Luccioni and Victor Schmidt and Thomas Dandres},
  title   = {Quantifying the Carbon Emissions of Machine Learning},
  journal = {arXiv preprint arXiv:1910.09700},
  year    = {2019}
}

@misc{deeplearningai2024federated,
  title        = {Introduction to Federated Learning},
  author       = {{DeepLearning.AI}},
  year         = {2024},
  howpublished = {\url{https://www.deeplearning.ai/short-courses/intro-to-federated-learning/}},
  note         = {Accessed: 2025-02-23}
}

@misc{xai2025grok3,
  title        = {Introducing Grok-3},
  author       = {{xAI}},
  year         = {2025},
  howpublished = {\url{https://x.ai/blog/grok-3}},
  note         = {Accessed: 2025-02-23}
}

@misc{nvidia2025,
  title        = {{NVIDIA GeForce RTX 5090 Specifications}},
  author       = {{NanoReview.net}},
  year         = {2025},
  howpublished = {\url{https://nanoreview.net/en/gpu/geforce-rtx-5090}},
  note         = {Accessed: 2025-02-23}
}

@misc{nanoreview2025,
  title        = {{NanoReview.net - Gadget Specifications and Comparisons}},
  author       = {{NanoReview.net}},
  year         = {2025},
  howpublished = {\url{https://nanoreview.net}},
  note         = {Accessed: 2025-02-23}
}

@misc{canalys2025,
  title        = {{Canalys Newsroom - Market Analysis and Research}},
  author       = {{Canalys}},
  year         = {2025},
  howpublished = {\url{https://canalys.com/newsroom}},
  note         = {Accessed: 2025-02-23}
}

% Model References

@article{yi2024phonelm,
  title   = {PhoneLM: A Small Language Model with Embedded Phone System App Capabilities},
  author  = {Yi, Chenxu and Zhai, Pan and Mathur, Varun and Singh, Param and Basu, Saibal and Hosseini, Hamed and Xu, Rui},
  journal = {arXiv preprint arXiv:2405.08131},
  year    = {2024}
}

@misc{llama3.2,
  title        = {Meta Releases Llama 3.2},
  author       = {Meta AI},
  howpublished = {\url{https://about.fb.com/news/2024/09/introducing-llama-3-2-1b-3b/}},
  year         = {2024}
}

@article{yang2024qwen2,
  title   = {Qwen2: Technical Report},
  author  = {Yang, Jinze and Wang, Shuai and Ma, Shuohang and Zheng, Jianbo and others},
  journal = {arXiv preprint arXiv:2404.05169},
  year    = {2024}
}

@article{bai2023qwentechnicalreport,
  title   = {Qwen Technical Report},
  author  = {Bai, Jinze and Wang, Shuai and Xiong, Fei and Hou, Zhenyu and others},
  journal = {arXiv preprint arXiv:2309.16609},
  year    = {2023}
}

@article{team2024gemma,
  title   = {Gemma: Open Models Based on Gemini Research and Technology},
  author  = {{Google Team}},
  journal = {arXiv preprint arXiv:2403.08295},
  year    = {2024}
}

@article{team2024gemma2,
  title        = {Gemma 2: Introducing the Next Generation of Gemma Open Models},
  author       = {{Google Team}},
  howpublished = {\url{https://blog.google/technology/developers/gemma-2/}},
  year         = {2024}
}

@article{allal2024SmolLM,
  title   = {SmolLM: A Small Language Model for Everyday Tasks},
  author  = {Allal, Lucas Benali and others},
  journal = {arXiv preprint arXiv:2407.12290},
  year    = {2024}
}

@article{pfeiffer2024h2o,
  title   = {H2O-Danube3: Raising the Bar in Open LLMs},
  author  = {Pfeiffer, Jonas and Dinu, Sebastian and others},
  journal = {arXiv preprint arXiv:2407.21587},
  year    = {2024}
}

@article{Rene,
  title   = {Rene: A Hybrid Model Architecture for Efficient Language Understanding},
  author  = {Wolf, Thomas and others},
  journal = {arXiv preprint arXiv:2405.07236},
  year    = {2024}
}

@article{hu2024minicpm,
  title   = {MiniCPM: Unveiling the Potential of Small Language Models},
  author  = {Hu, Edward and Huang, Wangchunshu and others},
  journal = {arXiv preprint arXiv:2402.03216},
  year    = {2024}
}

@article{du2024chinesetinyllmpretraining,
  title   = {Chinese TinyLLM: Pre-training and Fine-tuning of Low-Parameter Language Models for Chinese},
  author  = {Du, Yiheng and Wei, Furu},
  journal = {arXiv preprint arXiv:2404.05480},
  year    = {2024}
}

@article{groeneveld2024olmo,
  title   = {OLMo: Accelerating the Science of Language Models},
  author  = {Groeneveld, Dirk and Beltagy, Iz and Davis, Pete and others},
  journal = {arXiv preprint arXiv:2402.00838},
  year    = {2024}
}

@article{zhang2024tinyllamaopensourcesmalllanguage,
  title   = {TinyLlama: An Open-Source Small Language Model},
  author  = {Zhang, Peiyuan and Chen, Guangtao and others},
  journal = {arXiv preprint arXiv:2401.02385},
  year    = {2024}
}

@article{abdin2024phi,
  title   = {Phi-3: Building Better Language Models with More Data and Compute},
  author  = {Abdin, Nishant and Chaudhuri, Mostafa and others},
  journal = {arXiv preprint arXiv:2404.14219},
  year    = {2024}
}

@article{javaheripi2023phi,
  title   = {Phi-2: The Surprising Power of Small Language Models},
  author  = {Javaheripi, Mojan and Lobo, Jacob and others},
  journal = {arXiv preprint arXiv:2312.12397},
  year    = {2023}
}

@article{li2023textbooksneediiphi15,
  title   = {Textbooks Are All You Need II: phi-1.5 Technical Report},
  author  = {Li, Yuanzhi and Bubeck, Sébastien and others},
  journal = {arXiv preprint arXiv:2309.05463},
  year    = {2023}
}

@article{gunasekar2023textbooksneed,
  title   = {Textbooks Are All You Need},
  author  = {Gunasekar, Suriya and Zhang, Yi and others},
  journal = {arXiv preprint arXiv:2306.11644},
  year    = {2023}
}

@article{mehta2024openelm,
  title   = {OpenELM: An Efficient Language Foundation Model},
  author  = {Mehta, Harshit and Bartoldson, Brian and Shen, Jue and others},
  journal = {arXiv preprint arXiv:2404.17766},
  year    = {2024}
}



@article{liu2024mobilellm,
  title   = {MobileLLM: Empowering Mobile Large Language Models via Architectural Co-design},
  author  = {Liu, Ji and Chen, Yupeng and Liang, Zehao and others},
  journal = {arXiv preprint arXiv:2402.14905},
  year    = {2024}
}

@article{bellagente2024stable,
  title   = {Stable LM 2 1.6B Technical Report},
  author  = {Bellagente, Daniil and others},
  journal = {arXiv preprint arXiv:2405.10999},
  year    = {2024}
}

@misc{StableLM-3B-4E1T,
  author       = {{Stability AI}},
  title        = {StableLM-3B-4E1T: An open model trained on 1 trillion tokens of text},
  year         = {2023},
  howpublished = {\url{https://stability.ai/blog/stable-lm-3b-4e1t}}
}

@article{dey2023cerebras,
  title   = {Cerebras-GPT: A Family of Open, Compute-efficient, Large Language Models},
  author  = {Dey, Shreya and Chen, Tian and Wang, Ye and others},
  journal = {arXiv preprint arXiv:2304.03208},
  year    = {2023}
}

@article{biderman2023pythiasuiteanalyzinglarge,
  title   = {Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling},
  author  = {Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and others},
  journal = {arXiv preprint arXiv:2304.01373},
  year    = {2023}
}

@article{le2023bloom,
  title   = {BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  author  = {Le Scao, Teven and Fan, Angela and others},
  journal = {arXiv preprint arXiv:2211.05100},
  year    = {2023}
}

@article{taylor2022galactica,
  title   = {Galactica: A Large Language Model for Science},
  author  = {Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and others},
  journal = {arXiv preprint arXiv:2211.09085},
  year    = {2022}
}

@misc{gpt-neo,
  author       = {{EleutherAI}},
  title        = {GPT-Neo},
  year         = {2021},
  publisher    = {GitHub},
  howpublished = {\url{https://github.com/EleutherAI/gpt-neo}}
}

@article{shoeybi2019megatron,
  title   = {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author  = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and others},
  journal = {arXiv preprint arXiv:1909.08053},
  year    = {2019}
}

@article{zuo2024falcon,
  title   = {Falcon Mamba: Scaling Mamba with Facilitated Long Context},
  author  = {Zuo, Shuang and Jiang, Daxin},
  journal = {arXiv preprint arXiv:2405.11241},
  year    = {2024}
}

@article{jiang2023mistral,
  title   = {Mistral 7B},
  author  = {Jiang, Albert and Sablayrolles, Alexandre and others},
  journal = {arXiv preprint arXiv:2310.06825},
  year    = {2023}
}



@article{muralidharan2024compact,
  title   = {MINITRON: Efficient Heterogeneous Transformer via Pruning and Distillation},
  author  = {Muralidharan, Saket and others},
  journal = {arXiv preprint arXiv:2407.07900},
  year    = {2024}
}

@article{mitra2023orca,
  title   = {Orca 2: Teaching Small Language Models How to Reason},
  author  = {Mitra, Arindam and Mukherjee, Subhabrata and others},
  journal = {arXiv preprint arXiv:2311.11045},
  year    = {2023}
}

@article{zhang2023towards,
  title   = {Towards Making the Most of ChatGPT for Machine Translation},
  author  = {Zhang, Kehai and Chen, Zhuocheng and others},
  journal = {arXiv preprint arXiv:2309.02654},
  year    = {2023}
}

@misc{DatabricksBlog2023DollyV2,
  author       = {{Databricks}},
  title        = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM},
  year         = {2023},
  howpublished = {\url{https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm}}
}

@article{wu-etal-2024-lamini,
  title   = {LaMini-LM: A Diverse Herd of Distilled Models from LLaMa-2},
  author  = {Wu, Minghao and He, Haoran and Wang, Chengyu and Wang, Zhenyu and Wang, Yong and Xu, Guimin and others},
  journal = {arXiv preprint arXiv:2304.14402},
  year    = {2024}
}

@article{chung2024scaling,
  title   = {Scaling Instruction-Finetuned Language Models},
  author  = {Chung, Hyung Won and Hou, Le and others},
  journal = {arXiv preprint arXiv:2210.11416},
  year    = {2022}
}

@article{frantar2023sparsegpt,
  title   = {SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot},
  author  = {Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and others},
  journal = {arXiv preprint arXiv:2301.00774},
  year    = {2023}
}

@article{sun2024a,
  title   = {A Simple and Effective Pruning Approach for Large Language Models},
  author  = {Sun, Zongyu and Chen, Chen and Zhang, Zhitao and others},
  journal = {arXiv preprint arXiv:2306.11695},
  year    = {2023}
}

@article{zhang2023loraprune,
  title   = {LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning},
  author  = {Zhang, Yihang and Duan, Qihuang and Guo, Shengyuan and others},
  journal = {arXiv preprint arXiv:2305.18403},
  year    = {2023}
}

@article{men2024shortgpt,
  title   = {ShortGPT: Layers in Large Language Models are More Redundant Than You Expect},
  author  = {Men, Yu and Zhang, Xingyu and Sun, Ruiqi and others},
  journal = {arXiv preprint arXiv:2402.18952},
  year    = {2024}
}

@article{ma2023llm,
  title   = {LLM-Pruner: On the Structural Pruning of Large Language Models},
  author  = {Ma, Xinyin and Li, Gongfan and Cao, Xiaosong and others},
  journal = {arXiv preprint arXiv:2305.11627},
  year    = {2023}
}

@article{li2024nuteprune,
  title   = {NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models},
  author  = {Li, Xiaoqian and Sun, Yawen and Li, Yi and Shi, Tianrui and Jiang, Jing},
  journal = {arXiv preprint arXiv:2402.09773},
  year    = {2024}
}

@article{gao2024displlm,
  title   = {DISP-LLM: Data-Free Sparsity-Aware Large Language Model Compression},
  author  = {Gao, Hanchun and Han, Guangming and Wang, Xiangyu and others},
  journal = {arXiv preprint arXiv:2407.08293},
  year    = {2024}
}


@article{ma2024era,
  title   = {The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits},
  author  = {Ma, Shuming and Zhao, Hongyu and Xue, Lingxiao and others},
  journal = {arXiv preprint arXiv:2402.17764},
  year    = {2024}
}

@article{kim2024memory,
  title   = {Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization},
  author  = {Kim, Bingbing and Shen, Jie and Feng, Haici and Liu, Yunshui and others},
  journal = {arXiv preprint arXiv:2402.12317},
  year    = {2024}
}

@article{dettmers2024qlora,
  title   = {QLoRA: Efficient Finetuning of Quantized LLMs},
  author  = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal = {arXiv preprint arXiv:2305.14314},
  year    = {2023}
}

@article{kim2023squeezellm,
  title   = {SqueezeLLM: Dense-and-Sparse Quantization},
  author  = {Kim, Sehoon and Hooper, Coleman and Gholami, Amir and others},
  journal = {arXiv preprint arXiv:2306.07629},
  year    = {2023}
}

@article{guo2024compressing,
  title   = {Compressing Large Language Models by Joint Sparsification and Quantization},
  author  = {Guo, Mingjie and Jiang, Zhenyu and others},
  journal = {arXiv preprint arXiv:2405.04392},
  year    = {2024}
}

@article{adepu2024framequant,
  title   = {FrameQuant: Flexible Low-Bit Quantization for Transformers},
  author  = {Adepu, Sai Surya and Orshansky, Michael and others},
  journal = {arXiv preprint arXiv:2402.18150},
  year    = {2024}
}

@article{xu2024onebit,
  title   = {OneBit: Towards Extremely Low-bit Large Language Models},
  author  = {Xu, Yuzhuang and Lin, Min and Huang, Yangsibo and others},
  journal = {arXiv preprint arXiv:2402.11295},
  year    = {2024}
}

@article{huang2024billm,
  title   = {BiLLM: Pushing the Limit of Post-Training Quantization for LLMs},
  author  = {Huang, Han and Cai, Wei and Wang, Yuzhen and others},
  journal = {arXiv preprint arXiv:2402.04291},
  year    = {2024}
}

@article{zhang2024lqer,
  title   = {LQER: Low-Rank Quantization Error Reconstruction for LLMs},
  author  = {Zhang, Jing and Bai, Tianyi and Zhou, Wen and others},
  journal = {arXiv preprint arXiv:2403.17887},
  year    = {2024}
}

@article{hu2024llm,
  title   = {I-LLM: Integer-Only LLM Inference},
  author  = {Hu, Edward and Huang, Wangchunshu and others},
  journal = {arXiv preprint arXiv:2405.15496},
  year    = {2024}
}

@article{malinovskii2024pv,
  title   = {PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression},
  author  = {Malinovskii, Vahe and Kurin, Vitaly and others},
  journal = {arXiv preprint arXiv:2405.15548},
  year    = {2024}
}

@article{cho2024heterogeneouslorafederatedfinetuning,
  title   = {Heterogeneous LoRA for Federated Fine-tuning of Large Language Models},
  author  = {Cho, Moung and Cao, Xianbin and others},
  journal = {arXiv preprint arXiv:2402.07304},
  year    = {2024}
}

@article{yang2024llm,
  title   = {LLM-Neo: Harnessing the Power of Repetition with Neuron-emphasized Optimization},
  author  = {Yang, Dashuang and Yu, Zhiwen and Zhou, Kenji and others},
  journal = {arXiv preprint arXiv:2405.15756},
  year    = {2024}
}

% Dataset References

@article{li2024datacomp,
  title   = {Scaling Data-Constrained Language Models},
  author  = {Li, Niklas and others},
  journal = {arXiv preprint arXiv:2403.03915},
  year    = {2024}
}

@article{li2023starcodersourceyou,
  title   = {StarCoder: May the Source Be With You!},
  author  = {Li, Raymond and Choi, Daniel and Chung, Jordi and others},
  journal = {arXiv preprint arXiv:2305.06161},
  year    = {2023}
}

@article{paster2024openwebmath,
  title   = {OpenWebMath: A Large-Scale Web-based Mathematical Pretraining Corpus},
  author  = {Paster, Aditya and others},
  journal = {arXiv preprint arXiv:2401.10900},
  year    = {2024}
}

@misc{dolma,
  author       = {{AI2}},
  title        = {Dolma: An Open Corpus of High-Quality English Text for Language Model Pre-Training},
  year         = {2023},
  howpublished = {\url{https://huggingface.co/datasets/allenai/dolma}}
}

@article{benallal2024smollmcorpus,
  title   = {SmolLM-Corpus: An Open High-Quality Corpus for Pretraining Language Models},
  author  = {Benallal, Lucas and others},
  journal = {arXiv preprint arXiv:2406.11403},
  year    = {2024}
}

@article{gao2020pile,
  title   = {The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  author  = {Gao, Leo and Biderman, Stella and Black, Sid and others},
  journal = {arXiv preprint arXiv:2101.00027},
  year    = {2020}
}

@article{kocetkov2022stack,
  title   = {The Stack: 3 TB of Permissively Licensed Source Code},
  author  = {Kocetkov, Denis and Waites, Raymond and Lhoest, Quentin and others},
  journal = {arXiv preprint arXiv:2211.15533},
  year    = {2022}
}


@article{penedo2023refinedweb,
  title   = {The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data},
  author  = {Penedo, Guilherme and Crnisanin, Anis and Shen, Ethan and others},
  journal = {arXiv preprint arXiv:2306.01116},
  year    = {2023}
}

@article{together2023redpajama,
  title        = {RedPajama: An Open Source Recipe to Reproduce LLaMA training dataset},
  author       = {{Together Computer}},
  howpublished = {\url{https://github.com/togethercomputer/RedPajama-Data}},
  year         = {2023}
}

@article{gokaslan2019openwebtext,
  title        = {OpenWebText Corpus},
  author       = {Gokaslan, Aaron and Cohen, Vanya},
  howpublished = {\url{https://skylion007.github.io/OpenWebTextCorpus/}},
  year         = {2019}
}

@article{baumgartner2020pushshift,
  title   = {The Pushshift Reddit Dataset},
  author  = {Baumgartner, Jason and Zannettou, Savvas and Keegan, Brian and others},
  journal = {Proceedings of the International AAAI Conference on Web and Social Media},
  volume  = {14},
  pages   = {830--839},
  year    = {2020}
}

@article{trinh2018simple,
  title   = {A Simple Method for Commonsense Reasoning},
  author  = {Trinh, Trieu H and Le, Quoc V},
  journal = {arXiv preprint arXiv:1806.02847},
  year    = {2018}
}

@article{laurenccon2022bigscience,
  title   = {The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset},
  author  = {Laurençon, Hugo and Saulnier, Lucile and Wang, Thomas and others},
  journal = {arXiv preprint arXiv:2303.03915},
  year    = {2023}
}

@article{parmar2024nemotron,
  title   = {Nemotron-4: A Family of Open Large Language Models},
  author  = {Parmar, Niki and Siddhant, Aditya and Rose, Jared and others},
  journal = {arXiv preprint arXiv:2407.12034},
  year    = {2024}
}

@article{longpre2023flan,
  title   = {The Flan Collection: Designing Data and Methods for Effective Instruction Tuning},
  author  = {Longpre, Shayne and Hou, Le and Vu, Tu and others},
  journal = {arXiv preprint arXiv:2301.13688},
  year    = {2023}
}

@article{yuan2021wudaocorpora,
  title   = {WuDaoCorpora: A Super Large-scale Chinese Corpora for Pre-training Language Models},
  author  = {Yuan, Shuo and Zhao, Hai and Du, Zujie and others},
  journal = {AI Open},
  volume  = {2},
  pages   = {65--68},
  year    = {2021}
}

@article{cobbe2021gsm8k,
  title   = {Training Verifiers to Solve Math Word Problems},
  author  = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and others},
  journal = {arXiv preprint arXiv:2110.14168},
  year    = {2021}
}

@article{zhu2015aligning,
  title   = {Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},
  author  = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and others},
  journal = {Proceedings of the IEEE International Conference on Computer Vision},
  pages   = {19--27},
  year    = {2015}
}

@article{wang2018glue,
  title   = {GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author  = {Wang, Alex and Singh, Amanpreet and Michael, Julian and others},
  journal = {arXiv preprint arXiv:1804.07461},
  year    = {2018}
}

@article{lu2024small,
  title   = {Small language models: Survey, measurements, and insights},
  author  = {Lu, Zhenyan and Li, Xiang and Cai, Dongqi and Yi, Rongjie and Liu, Fangming and Zhang, Xiwen and Lane, Nicholas D and Xu, Mengwei},
  journal = {arXiv preprint arXiv:2409.15790},
  year    = {2024}
}

@article{van2024survey,
  title   = {A survey of small language models},
  author  = {Van Nguyen, Chien and Shen, Xuan and Aponte, Ryan and Xia, Yu and Basu, Samyadeep and Hu, Zhengmian and Chen, Jian and Parmar, Mihir and Kunapuli, Sasidhar and Barrow, Joe and others},
  journal = {arXiv preprint arXiv:2410.20011},
  year    = {2024}
}
@misc{together,
  author       = {Together AI},
  title        = {Together AI: The AI Acceleration Cloud},
  year         = {2023},
  howpublished = {\url{https://www.together.ai/}}
}

@misc{flock,
  author       = {FLock},
  title        = {FLock: Federated Machine Learning On the Blockchain},
  year         = {2023},
  howpublished = {\url{https://www.flock.io/}}
}


@misc{federatedscope,
  author       = {alibaba},
  title        = {FederatedScope: An easy-to-use federated learning platform},
  year         = {2024},
  howpublished = {\url{https://github.com/alibaba/FederatedScope}}
}

@misc{fedml,
  author       = {FedML-AI},
  title        = {FedML: The unified and scalable ML library for large-scale distributed training, model serving, and federated learning},
  year         = {2024},
  howpublished = {\url{https://github.com/FedML-AI/FedML}}
}

@article{ye2025fedllm,
  title   = {Fedllm-bench: Realistic benchmarks for federated learning of large language models},
  author  = {Ye, Rui and Ge, Rui and Zhu, Xinyu and Chai, Jingyi and Yaxin, Du and Liu, Yang and Wang, Yanfeng and Chen, Siheng},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {37},
  pages   = {111106--111130},
  year    = {2025}
}

@article{bolton2024biomedlm,
  title   = {Biomedlm: A 2.7 b parameter language model trained on biomedical text},
  author  = {Bolton, Elliot and Venigalla, Abhinav and Yasunaga, Michihiro and Hall, David and Xiong, Betty and Lee, Tony and Daneshjou, Roxana and Frankle, Jonathan and Liang, Percy and Carbin, Michael and others},
  journal = {arXiv preprint arXiv:2403.18421},
  year    = {2024}
}

@inproceedings{velasevic2023effects,
  title        = {On the effects of data heterogeneity on the convergence rates of distributed linear system solvers},
  author       = {Velasevic, Boris and Parasnis, Rohit and Brinton, Christopher G and Azizan, Navid},
  booktitle    = {2023 62nd IEEE Conference on Decision and Control (CDC)},
  pages        = {8394--8399},
  year         = {2023},
  organization = {IEEE}
}

@article{zhao2024retrieval,
  title   = {Retrieval-augmented mixture of lora experts for uploadable machine learning},
  author  = {Zhao, Ziyu and Gan, Leilei and Wang, Guoyin and Hu, Yuwei and Shen, Tao and Yang, Hongxia and Kuang, Kun and Wu, Fei},
  journal = {arXiv preprint arXiv:2406.16989},
  year    = {2024}
}

@misc{yao2024pursuit,
  title     = {The Pursuit of Fairness in Artificial Intelligence Models: A Survey},
  author        = {Yao, Yuxin and others},
  year          = {2024},
  eprint        = {2403.17333},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}

@misc{li2024carbon,
  title     = {Carbon Footprint Reduction for Sustainable Data Centers in Real-Time},
  author    = {Xiaoyu Li and others},
  year      = {2024},
  eprint    = {2403.14092},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AR}
  }

@misc{yang2024environmental,
  title     = {Environmental Burden of United States Data Centers in the Artificial Intelligence Era},
  author    = {Yang, Yuchen and others},
  year      = {2024},
  eprint    = {2411.09786},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AR}
}