<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Will LLMs Scaling Hit the Wall? Breaking Barriers with Distributed Resources on Massive Edge Devices">
  <meta property="og:title" content="Will LLMs Scaling Hit the Wall?"/>
  <meta property="og:description" content="Breaking Barriers with Distributed Resources on Massive Edge Devices"/>
  <meta property="og:url" content="https://tao.shen.zju.edu.cn/distributed-llm-edges/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/fedllm.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Will LLMs Scaling Hit the Wall?">
  <meta name="twitter:description" content="Breaking Barriers with Distributed Resources on Massive Edge Devices">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/fedllm.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="LLM, Edge Devices, Distributed Computing, Federated Learning, AI Democratization">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Will LLMs Scaling Hit the Wall?</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css" media="print" onload="this.media='all'">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" media="print" onload="this.media='all'">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Preload critical visualization resources -->
  <link rel="preload" href="htmls/iot_data_contribution_interactive.html" as="document">
  <link rel="preload" href="htmls/compute_trend_interactive.html" as="document">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js" defer></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js" defer></script>
  <script src="static/js/bulma-slider.min.js" defer></script>
  <script src="static/js/index.js" defer></script>
  
  <script>
    // Optimized iframe loading strategy
    document.addEventListener('DOMContentLoaded', function() {
      const iframes = document.querySelectorAll('iframe[loading="lazy"]');
      
      // Store original src to data attribute and remove from iframe
      iframes.forEach(iframe => {
        iframe.setAttribute('data-src', iframe.src);
        iframe.removeAttribute('src');
        iframe.classList.add('lazy-iframe');
      });
      
      // Function to load iframes when they come into view
      const loadIframes = () => {
        const lazyIframes = document.querySelectorAll('.lazy-iframe');
        
        lazyIframes.forEach(iframe => {
          const rect = iframe.getBoundingClientRect();
          // Load iframe when it's within 800px of the viewport
          if (rect.top <= window.innerHeight + 800 && rect.bottom >= -800) {
            iframe.src = iframe.getAttribute('data-src');
            iframe.classList.remove('lazy-iframe');
          }
        });
      };
      
      // Check for iframes on page load and scroll
      loadIframes();
      window.addEventListener('scroll', loadIframes);
      window.addEventListener('resize', loadIframes);
    });
  </script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Will LLMs Scaling Hit the Wall? Breaking Barriers with Distributed Resources on Massive Edge Devices</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://tao.shen" target="_blank">Tao Shen</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="#" target="_blank">Didi Zhu</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="#" target="_blank">Ziyu Zhao</a><sup>*</sup>,</span>
                    <span class="author-block">
                      <a href="#" target="_blank">Chao Wu</a>,</span>
                      <span class="author-block">
                        <a href="#" target="_blank">Fei Wu</a>
                      </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Zhejiang University</span>
                    <!-- <span class="author-block">Zhejiang University<br>ICML 2025 (Position Paper)</span> -->
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="icml25_position/example_paper.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Github link -->
                    <span class="link-block">
                      <a href="https://github.com/tao-shen/Distributed-LLM-Edges" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>


    <!-- Paper abstract -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                The remarkable success of foundation models has been driven by scaling laws, demonstrating that model performance improves predictably with increased <em>training data</em> and <em>model size</em>. 
                However, this scaling trajectory faces two critical challenges: the depletion of high-quality public data and the prohibitive computational power required for larger models, 
                which have been monopolized by tech giants. These two bottlenecks pose significant obstacles to the further development of AI.
              </p>
              <p>
                In this position paper, we argue that leveraging massive distributed edge devices can break through these barriers. 
                We reveal the vast untapped potential of data and computational resources on massive edge devices, and review recent technical advancements in distributed/federated learning that make this new paradigm viable.
                Our analysis suggests that by collaborating on edge devices, everyone can participate in training large language models with small edge devices.
                This paradigm shift towards distributed training on edge has the potential to democratize AI development and foster a more inclusive AI community.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper abstract -->


    <!-- Research Insights Section -->
    <section class="section">
      <div class="container is-max-desktop">
        <!-- The Challenge -->
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">The Challenge: Hitting the Wall</h2>
            <div class="content has-text-justified">
              <p>
                <strong>Data Depletion:</strong> Public data sources that have fueled AI development are becoming exhausted. High-quality data is increasingly scarce, with estimates suggesting human-generated public text data cannot sustain scaling beyond this decade. While synthetic data generation has been proposed as an alternative, AI-generated content may fail to yield performance improvements and risks polluting existing data sources.
              </p>
              
              <!-- Data Growth Visualization -->
              <div class="visualization-container">
                <iframe src="htmls/iot_data_contribution_interactive.html" width="100%" height="520px" frameborder="0" scrolling="no" class="visualization-iframe" loading="lazy"></iframe>
                <p class="has-text-centered">
                  <em>Figure 1: Global data volume is growing exponentially, with IoT devices contributing significantly to this growth.</em>
                </p>
              </div>
              
              <p>
                <strong>Computational Monopoly:</strong> The computational demands of training state-of-the-art models have become astronomical. Training GPT-4 reportedly required over 25,000 NVIDIA A100 GPUs, costing over $100M per training run. This level of computing power has become concentrated among a few tech giants, creating a monopolistic landscape that effectively excludes smaller companies and academic institutions from participating in foundational AI research.
              </p>
              
              <!-- Computing Trend Visualization -->
              <div class="visualization-container">
                <iframe src="htmls/compute_trend_interactive.html" width="100%" height="550px" frameborder="0" scrolling="no" class="visualization-iframe" loading="lazy"></iframe>
                <p class="has-text-centered">
                  <em>Figure 2: The computational demands of AI models have grown exponentially, approaching physical and economic limits.</em>
                </p>
              </div>
            </div>
          </div>
        </div>

        <!-- The Opportunity -->
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">The Opportunity: Edge Resources</h2>
            <div class="content has-text-justified">
              <p>
                <strong>Untapped Edge Data:</strong> Edge data generated from smartphones for the past 5 years are projected to reach 33.1 EB, offering fresh, diverse, and contextually rich training samples. This vastly exceeds the data used to train current state-of-the-art models.
              </p>
              
              <!-- Edge Computing and Smartphone Visualization -->
              <div class="visualization-container">
                <iframe src="htmls/edge_and_smartphone.html" width="100%" height="520px" frameborder="0" scrolling="no" class="visualization-iframe" loading="lazy"></iframe>
                <p class="has-text-centered">
                  <em>Figure 3: Smartphone data volume and edge computing market are expanding rapidly, offering vast untapped resources.</em>
                </p>
              </div>
              
              <p>
                <strong>Collective Computing Power:</strong> The combined computing power of edge devices - with smartphones delivering 9,278 EFLOPS over the past 5 years - demonstrates the feasibility of distributed model training. Training state-of-the-art models like DeepSeek-v3 would require only about 60,723 users with edge devices working in parallel to match its current training setup.
              </p>
              
              <!-- Smartphone Compute Trend -->
              <div class="visualization-container">
                <iframe src="htmls/smartphone_compute_trend.html" width="100%" height="520px" frameborder="0" scrolling="no" class="visualization-iframe" loading="lazy"></iframe>
                <p class="has-text-centered">
                  <em>Figure 4: The collective computing capabilities of smartphones have grown dramatically, providing a potential distributed training resource.</em>
                </p>
              </div>
            </div>
          </div>
        </div>

        <!-- Technical Approaches -->
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">Technical Approaches</h2>
            <div class="content has-text-justified">
              <p>
                <strong>Small Language Models at Edges:</strong> Deploying efficient, compact language models directly on edge devices reduces the computational and memory requirements while maintaining acceptable performance for many tasks.
              </p>
              <p>
                <strong>Collaborative Inference:</strong> Distributing the inference process across multiple devices allows for more complex models to be run without requiring any single device to handle the entire computational load.
              </p>
              <p>
                <strong>Collaborative Training:</strong> Federated learning enables model training across distributed devices without requiring data to leave the device, preserving privacy while leveraging the collective computational power of edge networks.
              </p>
              
              <!-- Federated LLM Visualization -->
              <div class="visualization-container">
                <iframe src="htmls/fedllm.html" width="100%" height="580px" frameborder="0" scrolling="no" class="visualization-iframe fedllm-viz" loading="lazy"></iframe>
                <p class="has-text-centered">
                  <em>Figure 5: Federated learning enables distributed training across edge devices while preserving privacy.</em>
                </p>
              </div>
            </div>
          </div>
        </div>

        <!-- Societal Impact -->
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">Societal Impact</h2>
            <div class="content has-text-justified">
              <p>
                <strong>AI Democratization:</strong> By breaking the monopoly on AI development, distributed edge computing can democratize access to AI technologies and empower a more diverse range of participants in the AI ecosystem.
              </p>
              <p>
                <strong>Privacy Preservation:</strong> Federated learning approaches keep sensitive data on user devices, addressing growing privacy concerns and complying with increasingly stringent data regulations.
              </p>
              <p>
                <strong>Environmental Benefits:</strong> Distributed training can significantly reduce the carbon footprint of AI development by utilizing existing devices rather than requiring dedicated energy-intensive data centers.
              </p>
              
              <!-- Yearly Mean Compute Trend -->
              <div class="visualization-container">
                <iframe src="htmls/compute_trend_yearly_mean.html" width="100%" height="520px" frameborder="0" scrolling="no" class="visualization-iframe" loading="lazy"></iframe>
                <p class="has-text-centered">
                  <em>Figure 6: The yearly growth in compute requirements shows the unsustainable trajectory of centralized AI training, highlighting the need for distributed approaches.</em>
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End Research Insights Section -->


    <!--BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@inproceedings{shen2025will,
  title={Will LLMs Scaling Hit the Wall? Breaking Barriers with Distributed Resources on Massive Edge Devices},
  author={Shen, Tao and Zhu, Didi and Zhao, Ziyu and Wu, Chao and Wu, Fei},
  booktitle={International Conference on Machine Learning},
  year={2025}
}</code></pre>
      </div>
    </section>
    <!--End BibTex citation -->


    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
                You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>

    <!-- Add some custom CSS for visualization containers -->
    <style>
      .visualization-container {
        margin: 2rem 0;
        padding: 1rem;
        background-color: #f9f9f9;
        border-radius: 8px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        overflow: hidden; /* Prevents any potential overflow */
        position: relative;
      }
      
      .visualization-container iframe {
        display: block;
        margin: 0 auto;
        max-width: none; /* 移除最大宽度限制 */
        border: none;
        transform: scale(0.75);
        transform-origin: 0 0;
        width: 200% !important; /* 进一步增加宽度，1/0.75 = 133% */
        height: 800px !important; /* 减小高度以减少空白空间 */
        margin-bottom: -180px; /* 负margin使caption上移 */
        will-change: transform; /* 优化渲染性能 */
      }

      /* 联邦学习图表专用样式 */
      .fedllm-viz {
        transform: scale(0.61) !important;
        width: 300% !important; /* 1/0.7 = 143% */
        height: 700px !important;
        margin-bottom: -300px !important;
        margin-top: -50px !important;
      }
      
      .visualization-container p {
        margin-top: 0; /* 移除caption顶部margin */
        position: relative; /* 相对定位以便与iframe重叠 */
        font-size: 0.9rem;
        z-index: 2; /* 确保caption在iframe之上 */
      }
      
      /* Lazy loading iframe placeholder */
      .lazy-iframe {
        background: #f5f5f5 url('data:image/svg+xml;charset=utf-8,%3Csvg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100"%3E%3Cpath fill="%23ccc" d="M50 25a3 3 0 100 50 3 3 0 000-50z"%3E%3CanimateTransform attributeName="opacity" from="1" to=".3" dur="0.8s" repeatCount="indefinite" /%3E%3C/path%3E%3C/svg%3E') no-repeat center center;
        background-size: 50px;
      }
      
      /* Responsive adjustments */
      @media screen and (max-width: 768px) {
        .visualization-iframe {
          transform: scale(0.65);
          width: 154% !important; /* 1/0.65 = 154% */
          height: 750px !important;
          margin-bottom: -80px; /* 为平板设备调整 */
        }
      }
      
      @media screen and (max-width: 480px) {
        .visualization-iframe {
          transform: scale(0.55);
          width: 182% !important; /* 1/0.55 = 182% */
          height: 850px !important;
          margin-bottom: -60px; /* 为手机设备调整 */
        }
      }
    </style>

  </body>
  </html>
